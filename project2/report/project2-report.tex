\documentclass[a4paper, twocolumn]{article}

% Included packages ---------------------------------------------------------- %
\usepackage{lipsum}                          % Generate random, blind, filler-text.
\usepackage[utf8]{inputenc}                  % utf-8 encoding, æ, ø , å, etc.
\usepackage{a4wide}                          % Adjust margins to better fit A4 format.
\usepackage{array}                           % Matrices.
\usepackage{dsfont}                          % Math symbols.
\usepackage{amsmath}                         % Math symbols, and enhanced matrices.
\usepackage{amsfonts}                        % Math fonts.
\usepackage{amssymb}                         % Additional symbols.
\usepackage{mathrsfs}                        % Most additional symbols.
\usepackage[pdftex]{graphicx}                % Improved inclusion of .pdf-graphics files.
\usepackage{sidecap}                         % Floats with captions to the right/left.
\usepackage{enumerate}                       % Change counters (arabic, roman, etc.).
\usepackage{floatrow}                        % Multi-figure floats.
\usepackage{subfig}                          % Multi-figure floats.
\usepackage{bm}                              % Bolded text in math mode.
\usepackage[framemethod=default]{mdframed}   % Make boxes.
\usepackage{listings}                        % For including source code.
\usepackage{mathtools}                       % Underbrackets, overbrackets.
\usepackage[dvipsnames]{xcolor}              % Colors.
\usepackage{capt-of}                         % Caption things which are not floats.
\usepackage{fontawesome}                     % Github icon, etc. \faGithub
\usepackage{sidecap}                         % Floats with captions on the side.
\usepackage{tabularx}                        % Tables and stuff.
\usepackage{tabulary}                        % Tables and stuff.
\usepackage[sf,sl,outermarks]{titlesec}      % Change fonts in section{}, subsection{}, etc.
\usepackage[subfigure]{tocloft}              % Change spacing between numbers and titles in TOC.
\usepackage{booktabs}                        % \toprule, \midrule, etc. for tables.
\usepackage{siunitx}                         % Allows S table column, aligning on decimal point.
\usepackage{chngcntr}                        % Change counter behaviour, supress increment of sub counters.
\usepackage[%                                % Adds functionality to captions.
  tableposition = top,
  labelsep      = period,
  justification = raggedright,
  format        = hang,
  ]{caption}                                 
\usepackage[%                                % Interactive references and links, colored.
  colorlinks  = true,
  linkcolor   = black,
  urlcolor    = blue,
  citecolor   = black,
  linktocpage = true,
  ]{hyperref}            
\usepackage[%                                % References, in super-script form.
  autocite    = superscript,
  backend     = biber,
  sortcites   = true,
  style       = numeric-comp,
  sorting     = none,
  url         = false,
  ]{biblatex}
\usepackage[autostyle, english = american]{csquotes} % Assure quotation marks are inserted correctly aligned left/right.
\MakeOuterQuote{"}

% Package settings ----------------------------------------------------------- %
\renewcommand{\thesection}{\Roman{section}}         % I, II, III, IV, etc. section numbering
\renewcommand{\thesubsection}{\Alph{subsection}}    % A, B, C, etc. subsection numbering
\renewcommand{\thesubsubsection}{}                  % Remove subsubsection numbering.
\floatsetup[table]{capposition=top}                 % Place table captions above the table.
\captionsetup[subfigure]{labelformat=empty}         % Remove the (a), (b), etc. tags from subfigures.
\advance\cftsecnumwidth 1.0em\relax                 % Set the spacing between section headings and titles in TOC with tocloft.
\advance\cftsubsecindent 1.0em\relax                % Set the spacing between subsection headings and titles in TOC with tocloft.
\advance\cftsubsecnumwidth 1.0em\relax              % Set the spacing between subsubsection headings and titles in TOC with tocloft.
\newcommand{\listingsfont}{\ttfamily}
\newcommand{\inlinepy}[1]{\lstinline[language={python}]{#1}}
\newcommand{\inlinecc}[1]{\lstinline[language={c++}]{#1}}
\counterwithout*{subsection}{section}               % Dont reset the subsection counter on new \section{} calls.
\renewcommand{\figurename}{FIG.}                    % Captions of figures read FIG.
\renewcommand{\tablename}{TABLE}                    % Captions of tables read TABLE 
\renewcommand{\thetable}{\Roman{table}}             % Number tables with roman numerals.

% Section headings settings -------------------------------------------------- %
\titleformat{\section}[hang]  % {command}[shape]
  {\normalfont\bfseries}      % {format}
  {\thesection.}              % {label}
  {2ex}                       % {sep}
  {\centering\MakeUppercase}  % {before-code}[after-code]

\titleformat{\subsection}[hang] % {command}[shape]
  {\normalfont\bfseries}        % {format}
  {\thesubsection.}             % {label}
  {1ex}                         % {sep}
  {\centering}                  % {before-code}[after-code]

\titleformat{\subsubsection}[hang]  % {command}[shape]
  {\normalfont\bfseries}            % {format}
  {}                                % {label}
  {1ex}                             % {sep}
  {\centering}                      % {before-code}[after-code]


% References ----------------------------------------------------------------- %
\newcommand{\Fig}[1]{Fig.\ \ref{fig:#1}}
\newcommand{\fig}[1]{Fig.\ \ref{fig:#1}}
\newcommand{\eq} [1]{Eq.\ (\ref{eq:#1})}
\newcommand{\Eq} [1]{Eq.\ (\ref{eq:#1})}
\newcommand{\tab}[1]{Table \ref{tab:#1}}
\newcommand{\Tab}[1]{Table \ref{tab:#1}}

% Matrices ------------------------------------------------------------------- %
\newcommand{\mat} [2]{\begin{matrix}[#1] #2 \end{matrix}}    % Nothing enclosing it.
\newcommand{\pmat}[2]{\begin{pmatrix}[#1] #2 \end{pmatrix}}  % Enclosing parentheses.
\newcommand{\bmat}[2]{\begin{bmatrix}[#1] #2 \end{bmatrix}}  % Enclosing square brackets.
\newcommand{\vmat}[2]{\begin{vmatrix}[#1] #2 \end{vmatrix}}  % Enclosing vertical bars.
\newcommand{\Vmat}[2]{\begin{Vmatrix}[#1] #2 \end{Vmatrix}}  % Enclosing double bars.

% Manually set alignment of rows / columns in matrices (mat, pmat, etc.) ----- %
\makeatletter
\renewcommand*\env@matrix[1][*\c@MaxMatrixCols c]{%
  \hskip -\arraycolsep
  \let\@ifnextchar\new@ifnextchar
  \array{#1}}
\makeatother

% figures in multicols environment ------------------------------------------- %
\newenvironment{Figure}
  {\par\medskip\noindent\minipage{\linewidth}}
  {\endminipage\par\medskip}

% Set bibliography file and path for images.
\addbibresource{../ref/project1-references.bib}
\bibliography{../ref/project2-references.bib}
\graphicspath{{../figures/}}

% Black frame with gray background ------------------------------------------ %
\definecolor{gray}{gray}{0.9}
\newmdenv[linecolor=white,backgroundcolor=gray]{grayframe}


% Title
\title{{\sc Machine learning applied to the one- and two dimensional Ising model. \\ {\large FYS-STK4155: Project 2}}}
\author{Morten Ledum \& Håkon Kristiansen \\ \faGithub \ {\small \href{https://github.com/mortele/FYS-STK4155/tree/master/project2}{github.com/mortele/FYS-STK4155}}}
% ---------------------------------------------------------------------------- %
% ---------------------------------------------------------------------------- %
\begin{document}


\twocolumn[
  \begin{@twocolumnfalse}
\maketitle

\begin{abstract}

\end{abstract}

\tableofcontents 
  \end{@twocolumnfalse}
]

\newpage

\section{Introduction}
\lipsum[1]
\autocite{trevor2009elements}
\section{Theory}
In the following we outline the theory of the present work. We 
consider logistic regression as a model for classification problems. Furthermore, neural networks are discussed 
both in the context of regression analysis and classification. The theoretical aspects of linear regression have been 
discussed in previous work and is not repeated here.

In contrast to the linear regression model, we can not find the optimal parameters of the logistic or neural network 
models analytically. Thus, we have ro rely on numerical methods for optimization. In particular we will give a brief summary
gradient descent methods.

\subsection{Logistic regression}
\lipsum[3]
\subsection{Neural networks}
\lipsum[4]

\subsection{Gradient Descent}
Almost every problem in machine learning and data science starts
with a dataset $X$, a model $g(\theta)$, which is a function of the parameters $\theta$ and a cost 
function $C(X, g(\theta))$ that allows us to judge how well the
model $g(\theta)$ explains the observations $X$. The model is fit by finding the values of $\theta$ that minimize the 
cost function. Ideally we would be able to solve for $\theta$ analytically, however this is not possible in general and 
we must use numerical methods to compute the minimum.
\subsubsection{The method of steepest descent}
The basic idea of gradient descent is that a function $F(\mathbf{x})$, $ \mathbf{x} \equiv (x_1,\cdots,x_n)$, 
decreases fastest if one goes from $\bf {x}$ in the direction of the negative gradient $-\nabla F(\mathbf{x})$.
It can be shown that if 
\begin{equation}
\mathbf{x}_{k+1} = \mathbf{x}_k - \gamma_k \nabla F(\mathbf{x}_k), \ \ \gamma_k > 0
\end{equation}
for $\gamma_k$ small enough, then $F(\mathbf{x}_{k+1}) \leq F(\mathbf{x}_k)$. This means that for a sufficiently 
small $\gamma_k$ we are always moving towards smaller function values, i.e a minimum. 

This observation is the basis of the method of steepest descent, which is also referred to as just gradient descent (GD). 
One starts with an initial guess $\mathbf{x}_0$ for a minimum of $F$ and compute new approximations according to
\begin{equation}
\mathbf{x}_{k+1} = \mathbf{x}_k - \gamma_k \nabla F(\mathbf{x}_k), \ \ k \geq 0.
\end{equation}
The parameter $\gamma_k$ is often referred to as the step length or the learning rate in the context of ML.

Ideally the sequence $\{ \mathbf{x}_k \}_{k=0}$ converges to a \textit{global} minimum of the function $F$. 
In general we do not know if we are in a global or local minimum.  
In the special case when $F$ is a \textit{convex function, all local minima are also global minima}, so in this case gradient 
descent can converge to the global solution. The advantage of this scheme is that it is conceptually simple and 
straightforward to implement. 

However the method in this form has some severe limitations:

\begin{itemize}
 \item In machine learing we are often faced with non-convex high dimensional cost functions with many local minimum. 
 Since GD is deterministic we will get stuck in a local minimum, if the method converges, unless we have a very good intial 
 guess. This also implies that the scheme is sensitive to the chosen initial condition.
 \item Note that gradient is a function of $\mathbf{x} = (x_1,\cdots,x_n)$ which makes it expensive to compute numerically.
 \item GD is sensitive to the choice of learning rate $\gamma_k$. This is due to the fact that we are only guaranteed 
that $F(\mathbf{x}_{k+1}) \leq F(\mathbf{x}_k)$ for \textit{sufficiently} small $\gamma_k$. The problem is to determine an 
optimal learning rate. If the learning rate is chosen to small the method will take a long to converge and if it is to 
large we can experience erratic behavior.
\item Many of these shortcomings can be alleviated by introducing randomness. One such method is that of Stochastic Gradient Descent 
(SGD).
\end{itemize}

\subsubsection{Stochastic Gradient Descent}
Stochastic gradient descent (SGD) and variants thereof address some of the shortcomings of the Gradient descent method discussed above. 

The underlying idea of SGD comes from the observation that the cost function, which we want to minimize, can almost always be 
written as a sum over $n$ datapoints $\{\mathbf{x}_i\}_{i=1}^n$,
\begin{equation}
 C(\mathbf{\theta}) = \sum_{i=1}^n c_i(\mathbf{x}_i, \mathbf{\theta}). 
\end{equation}
This in turn means that the gradient can be computed as a sum over $i$-gradients 
\begin{equation}
\nabla_\theta C(\mathbf{\theta}) = \sum_i^n \nabla_\theta c_i(\mathbf{x}_i, \mathbf{\theta}).  
\end{equation}
Now, stochasticity/randomness is introduced by only taking the gradient on a subset of the data called minibatches.  
If there are $n$ datapoints and the size of each minibatch is $M$, there will be $n/M$ minibatches. We denote these 
minibatches by $B_k$ where $k=1,\cdots,n/M$. 

As an example, suppose we have $10$ datapoints $( \mathbf{x}_1, \cdots, \mathbf{x}_{10} )$ and we choose to have $M=5$
minibathces, then each minibatch contains two datapoints. In particular we have 
$B_1 = (\mathbf{x}_1,\mathbf{x}_2), \cdots, B_5 = (\mathbf{x}_9,\mathbf{x}_{10})$. Note that if you choose $M=1$ you have 
only a single batch with all datapoints and on the other extreme, you may choose $M=n$ resulting in a minibatch for each 
datapoint, i.e $B_k = \mathbf{x}_k$.

The idea is now to approximate the gradient by replacing the sum over all datapoints with a sum over the datapoints in one the 
minibatches picked at random in each gradient descent step
\begin{align}\nabla_\theta C(\mathbf{\theta}) = &\sum_{i=1}^n \nabla_\theta c_i(\mathbf{x}_i, \mathbf{\theta}) \nonumber \\
\rightarrow &\sum_{i \in B_k}^n \nabla_\theta c_i(\mathbf{x}_i, \mathbf{\theta}).
\end{align}
                                                                                                                                                                                                 
Thus a gradient descent step now looks like 
\begin{equation} \theta_{j+1} = \theta_j - \gamma_j \sum_{i \in B_k}^n \nabla_\theta c_i(\mathbf{x}_i, \mathbf{\theta}) \end{equation}
where $k$ is picked at random with equal probability from the interval $[1,n/M]$. An iteration over the number of 
minibathces $n/M$ is commonly referred to as an epoch. Thus it is typical to choose a number of epochs and for each epoch 
iterate over the number of minibatches.
 
Taking the gradient only on a subset of the data has two important benefits. First, it introduces randomness 
which decreases the chance that our opmization scheme gets stuck in a local minima. Second, if the size of the 
minibatches are small relative to the number of datapoints ($M < n$), the computation of the gradient is much cheaper 
since we sum over the datapoints in the k-th minibatch and not all $n$ datapoints. 

A natural question is when do we stop the search for a new minimum? One possibility is to compute the full gradient after a 
given number of epochs and check if the norm of the gradient is smaller than some threshold and stop if true. However, 
the condition that the gradient is zero is valid also for local minima, so this would only tell us that we are close to a 
local/global minimum. However, we could also evaluate the cost function at this point, store the result and continue 
the search. If the test kicks in at a later stage we can compare the values of the cost function and keep the $\theta$ that 
gave the lowest value. 

Another approach is to let the step length $\gamma_j$ depend on the number of epochs in such a way that it becomes very small 
after a reasonable time such that we do not move at all. 

As an example, let $e = 0,1,2,3,\cdots$ denote the current epoch and let $t_0, t_1 > 0$ be two fixed numbers. Furthermore, 
let $t = e \cdot m + i$ where $m$ is the number of minibatches and $i=0,\cdots,m-1$. Then the function 
\begin{equation}\gamma_j(t; t_0, t_1) = \frac{t_0}{t+t_1} \end{equation}
goes to zero as the number of epochs gets large. I.e. we start with a step length $\gamma_j (0; t_0, t_1) = t_0/t_1$ which 
decays in "time" t. 

In this way we can fix the number of epochs, compute $\theta$ and evaluate the cost function at the end. Repeating the 
computation will give a different result since the scheme is random by design. Then we pick the final $\theta$ that gives 
the lowest value of the cost function.

\section{Model systems}
In this work we apply the machine learning algorithms discussed to the one- and two-dimensional Ising model. 
Linear regression and neural networks are used to estimate the coupling constant of the one-dimensional Ising model.

The two-dimensional Ising model is known to show phase transition. In particular, these phases can be labeled as ordered 
or disordered. Thus it is interesting to investigate whether logistic regression or neural networks can be trained to to 
classify the phases. 

\subsection{The one-dimensional Ising model}
\lipsum[6]
\subsection{The two-dimensional Ising model}
\lipsum[7]
\section{Results and discussion}
\lipsum[8]
\subsection{Learning the one-dimensional Ising Hamiltonian}
\lipsum[9]
\subsection{Classifying phases of the two-dimensional Ising model}
\lipsum[10]
\section{Conclusion}
\lipsum[11]

\onecolumn{
\printbibliography
}

\end{document}



