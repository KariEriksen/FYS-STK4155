\documentclass[a4paper, twocolumn]{article}

% Included packages ---------------------------------------------------------- %
\usepackage{lipsum}                          % Generate random, blind, filler-text.
\usepackage[utf8]{inputenc}                  % utf-8 encoding, æ, ø , å, etc.
\usepackage{a4wide}                          % Adjust margins to better fit A4 format.
\usepackage{array}                           % Matrices.
\usepackage{dsfont}                          % Math symbols.
\usepackage{amsmath}                         % Math symbols, and enhanced matrices.
\usepackage{amsfonts}                        % Math fonts.
\usepackage{amssymb}                         % Additional symbols.
\usepackage{mathrsfs}                        % Most additional symbols.
\usepackage[pdftex]{graphicx}                % Improved inclusion of .pdf-graphics files.
\usepackage{sidecap}                         % Floats with captions to the right/left.
\usepackage{enumerate}                       % Change counters (arabic, roman, etc.).
\usepackage{floatrow}                        % Multi-figure floats.
\usepackage{subfig}                          % Multi-figure floats.
\usepackage{bm}                              % Bolded text in math mode.
\usepackage[framemethod=default]{mdframed}   % Make boxes.
\usepackage{listings}                        % For including source code.
\usepackage{mathtools}                       % Underbrackets, overbrackets.
\usepackage[dvipsnames]{xcolor}              % Colors.
\usepackage{capt-of}                         % Caption things which are not floats.
\usepackage{algorithm2e}                     % Algorithm non-float which we can caption by \captionof{algocf}{<caption>}.
\usepackage{fontawesome}                     % Github icon, etc. \faGithub
\usepackage{sidecap}                         % Floats with captions on the side.
\usepackage{tabularx}                        % Tables and stuff.
\usepackage{tabulary}                        % Tables and stuff.
\usepackage[sf,sl,outermarks]{titlesec}      % Change fonts in section{}, subsection{}, etc.
\usepackage[subfigure]{tocloft}              % Change spacing between numbers and titles in TOC.
\usepackage{booktabs}                        % \toprule, \midrule, etc. for tables.
\usepackage{siunitx}                         % Allows S table column, aligning on decimal point.
\usepackage{chngcntr}                        % Change counter behaviour, supress increment of sub counters.
\usepackage{tikz}                            % Draw complicated pictures in a super hard way.
\usepackage{cuted}                           % Place long equations in @twocolumn over entire page.
\usepackage[%                                % Adds functionality to captions.
  tableposition = top,
  labelsep      = period,
  justification = raggedright,
  format        = hang,
  ]{caption}                                 
\usepackage[%                                % Interactive references and links, colored.
  colorlinks  = true,
  linkcolor   = black,
  urlcolor    = blue,
  citecolor   = black,
  linktocpage = true,
  ]{hyperref}            
\usepackage[%                                % References, in super-script form.
  autocite    = superscript,
  backend     = biber,
  sortcites   = true,
  style       = numeric-comp,
  sorting     = none,
  url         = false,
  ]{biblatex}
\usepackage[autostyle, english = american]{csquotes} % Assure quotation marks are inserted correctly aligned left/right.
\MakeOuterQuote{"}

% Package settings ----------------------------------------------------------- %
\renewcommand{\thesection}{\Roman{section}}         % I, II, III, IV, etc. section numbering
\renewcommand{\thesubsection}{\Alph{subsection}}    % A, B, C, etc. subsection numbering
\renewcommand{\thesubsubsection}{}                  % Remove subsubsection numbering.
\floatsetup[table]{capposition=top}                 % Place table captions above the table.
\captionsetup[subfigure]{labelformat=empty}         % Remove the (a), (b), etc. tags from subfigures.
\advance\cftsecnumwidth 1.0em\relax                 % Set the spacing between section headings and titles in TOC with tocloft.
\advance\cftsubsecindent 1.0em\relax                % Set the spacing between subsection headings and titles in TOC with tocloft.
\advance\cftsubsecnumwidth 1.0em\relax              % Set the spacing between subsubsection headings and titles in TOC with tocloft.
\newcommand{\listingsfont}{\ttfamily}
\newcommand{\inlinepy}[1]{\lstinline[language={python}]{#1}}
\newcommand{\inlinecc}[1]{\lstinline[language={c++}]{#1}}
\counterwithout*{subsection}{section}               % Dont reset the subsection counter on new \section{} calls.
\renewcommand{\figurename}{FIG.}                    % Captions of figures read FIG.
\renewcommand{\tablename}{TABLE}                    % Captions of tables read TABLE 
\renewcommand{\thetable}{\Roman{table}}             % Number tables with roman numerals.
\usetikzlibrary{matrix}                             % Some tikz picture library things.
\usetikzlibrary{arrows.meta, calc, chains, positioning}
\definecolor{listingsbackgroundcolor}{rgb}{0.975,0.975,0.975}
\colorlet{shadecolor}{listingsbackgroundcolor}



% Section headings settings -------------------------------------------------- %
\titleformat{\section}[hang]  % {command}[shape]
  {\normalfont\bfseries}      % {format}
  {\thesection.}              % {label}
  {2ex}                       % {sep}
  {\centering\MakeUppercase}  % {before-code}[after-code]

\titleformat{\subsection}[hang] % {command}[shape]
  {\normalfont\bfseries}        % {format}
  {\thesubsection.}             % {label}
  {1ex}                         % {sep}
  {\centering}                  % {before-code}[after-code]

\titleformat{\subsubsection}[hang]  % {command}[shape]
  {\normalfont\bfseries}            % {format}
  {}                                % {label}
  {1ex}                             % {sep}
  {\centering}                      % {before-code}[after-code]


% References ----------------------------------------------------------------- %
\newcommand{\Fig}[1]{Fig.\ \ref{fig:#1}}
\newcommand{\fig}[1]{Fig.\ \ref{fig:#1}}
\newcommand{\eq} [1]{Eq.\ (\ref{eq:#1})}
\newcommand{\Eq} [1]{Eq.\ (\ref{eq:#1})}
\newcommand{\tab}[1]{Table \ref{tab:#1}}
\newcommand{\Tab}[1]{Table \ref{tab:#1}}

% Matrices ------------------------------------------------------------------- %
\newcommand{\mat} [2]{\begin{matrix}[#1] #2 \end{matrix}}    % Nothing enclosing it.
\newcommand{\pmat}[2]{\begin{pmatrix}[#1] #2 \end{pmatrix}}  % Enclosing parentheses.
\newcommand{\bmat}[2]{\begin{bmatrix}[#1] #2 \end{bmatrix}}  % Enclosing square brackets.
\newcommand{\vmat}[2]{\begin{vmatrix}[#1] #2 \end{vmatrix}}  % Enclosing vertical bars.
\newcommand{\Vmat}[2]{\begin{Vmatrix}[#1] #2 \end{Vmatrix}}  % Enclosing double bars.

% Manually set alignment of rows / columns in matrices (mat, pmat, etc.) ----- %
\makeatletter
\renewcommand*\env@matrix[1][*\c@MaxMatrixCols c]{%
  \hskip -\arraycolsep
  \let\@ifnextchar\new@ifnextchar
  \array{#1}}
\makeatother

% figures in multicols environment ------------------------------------------- %
\newenvironment{Figure}
  {\par\medskip\noindent\minipage{\linewidth}}
  {\endminipage\par\medskip}

% Set bibliography file and path for images.
\addbibresource{../ref/project2-references.bib}
\bibliography{../ref/project2-references.bib}
\graphicspath{{../figures/}}

% Black frame with gray background ------------------------------------------ %
\definecolor{gray}{gray}{0.9}
\newmdenv[linecolor=white,backgroundcolor=gray]{grayframe}
\newmdenv[linecolor=white,backgroundcolor=shadecolor]{shadeframe}

% Title
\title{{\sc Machine learning applied to the one- and two dimensional Ising model. \\ {\large FYS-STK4155: Project 2}}}
\author{Morten Ledum \& Håkon Kristiansen \\ \faGithub \ {\small \href{https://github.com/mortele/FYS-STK4155/tree/master/project2}{github.com/mortele/FYS-STK4155}}}
% ---------------------------------------------------------------------------- %
% ---------------------------------------------------------------------------- %
\begin{document}


\twocolumn[
  \begin{@twocolumnfalse}
\maketitle

\begin{abstract}

\end{abstract}

\tableofcontents 
  \end{@twocolumnfalse}
]

\newpage


\section{Introduction}
There are many problems that require a probability estimate as output. 
This could for example predicting whether a 
person will develop a specific disease given genetic information. Another example, which we will examine closer, is to 
predict if a given spin-configuration generated from the two-dimension Ising model is ordrered or disordered. Problems of this 
type are referred to as \textit{classification} problems. 

Classification is fundamentally different from the regression problems we studied previously, 
in the sense that the predicted outcome only takes values across discrete categories. 
Thus, we will need different tools than that of linear regression. In this work we first consider 
\textit{logistic regression} as a method for classification. 

Somethingsomething background deep neural networks 

Apply it to one-dim Ising and classification of two-dim Ising.

\section{Theory}
In the following we outline the theory of the present work. We 
consider logistic regression as a model for classification problems. Furthermore, neural networks are discussed 
both in the context of regression analysis and classification. The theoretical aspects of linear regression have been 
discussed in previous work and is not repeated here.

In contrast to the linear regression model, we can not find the optimal parameters of the logistic or neural network 
models analytically. Thus, we have ro rely on numerical methods for optimization. In particular we will give a brief summary
gradient descent methods.

\subsection{Logistic regression}
Suppose that we are given a dataset $\{ (\mathbf{x}^{(i)}, y_i) \}_{i=1}^n $ where we have $p$ predictors 
for each data sample $\mathbf{x}^{(i)} = \{ x_1^{(i)}, \cdots, x_p^{(i)} \}$. The responses/outcomes $y_i$ are discrete and can 
only take values from $k=0,1,\dots,K-1$ (i.e $K$ classes). The goal is to predict the output classes given $n$ samples each containing 
$p$ predictors. Throughout this section we assume that there are just two possible outcomes, i.e $y_i \in \{0,1\}$.

In logistic regression, in contrast to linear regressions, we model the \textit{probability} that $y_i$ belongs 
to class $1$, given $\mathbf{x}^{(i)}$. Let $p(y|x)$ denote the probability of event $y$ given $x$,
then the \textit{logistic model} is 
\begin{align}
 p \left(y=1 | \mathbf{x}; \beta \right) &= \frac{1}{1+e^{-\beta \cdot \mathbf{x}}} \\
 p \left(y=0 | \mathbf{x}; \beta \right) &= 1 - p \left(y=1 | \mathbf{x}; \beta \right).
\end{align}
Here $\beta = (\beta_0, \beta_1, \dots, \beta_p)$ are the parameters of the model. Note the appearance of the intercept 
term $\beta_0$. In order to keep notation compact $\mathbf{x}^{(i)}$ can be augmented to incorporate the intercept by adding 
a $1$ to each sample, i.e 
\begin{equation*}
 \mathbf{x}^{(i)} \rightarrow \{ 1, x_1^{(i)}, \dots, x_p^{(i)} \}.
\end{equation*}
The term $\beta \cdot \mathbf{x} = \beta_0 + \sum_{k=1}^p \beta_k x_k $ is known as the \textit{log-odds} and the function 
\begin{equation}
\sigma(\beta \cdot \mathbf{x}) = \frac{1}{1+e^{-\beta \cdot \mathbf{x}}} 
\end{equation}
is called the \textit{sigmoid} of $\beta \cdot \mathbf{x}$. Also note that the sigmoid satisfies 
\begin{align}
 \lim_{t \rightarrow \infty} \sigma(t) &= 1 \\
 \lim_{t \rightarrow -\infty} \sigma(t) &= 0
\end{align}
which justifies its use as a model for probabilities.

The logistic model can now be used for classification by predicting a class using the estimated probabilities according to 
\begin{equation}
 \hat{y}_i = \begin{cases}
            1 \ \ \text{if }  p \left(y=1 | \mathbf{x}^{(i)} \right) \geq 0.5 \\
            0 \ \ \text{if }  p \left(y=1 | \mathbf{x}^{(i)} \right) < 0.5. \\
           \end{cases}
\end{equation}
\subsubsection{Training the logistic model}
How do we train the logistic model? The answer is to use the prinicple of \textit{maximum likelihood}. Under the assumption 
that every sample $\mathbf{x}^{(i)}$ is indenpendent, the likelihood is given by 
\begin{align}
 L(\beta) &= \prod_{i : y_i = 1} p(y_i=1 | \mathbf{x}^{(i)}) \prod_{i : y_i = 0} p(y_i=0 | \mathbf{x}^{(i)}) \nonumber \\
 &= \prod_{i=1}^n p(y_i=1 | \mathbf{x}^{(i)})^{y_i} ( 1-p(y_i=1 | \mathbf{x}^{(i)}))^{1-y_i}.
\end{align}
Then, the parameters $\beta$ are chosen to maximize the likelihood. 

It turns out that it is easier to work with the \textit{log-likelihood}
\begin{align}
 l(\beta) &= \log(L(\beta)) \nonumber \\
 &= \sum_{i=1}^n y_i p(y_i=1 | \mathbf{x}^{(i)}) + (1-y_i)( 1-p(y_i=1 | \mathbf{x}^{(i)}).
\end{align}
Maximizing the logarithm of a function is equivalent to maximizing the function itself. 

In order to see this, let $f(x)$ be a
real valued function and let $x^*$ be a maximum point of $f(x)$, i.e 
\begin{equation}
 f'(x^*) = 0, \ \ f''(x^*) < 0.
\end{equation}
Furthermore, assume that $f(x) > 0$ and consider $\log(f(x))$. Taking derivatives we have that 
\begin{align}
 &\frac{d}{dx} \log (f(x)) = \frac{f'(x)}{f(x)} \\
 \Rightarrow &\frac{d}{dx} \log(f(x^*)) = 0 \\
 &\frac{d^2}{dx^2} \log(f(x)) = \frac{f''(x) f(x) - f'(x)^2}{f(x)^2} \\
 \Rightarrow &\frac{d^2}{dx^2} \log(f(x^*)) < 0, 
\end{align}
where the last inequality follows from the fact that we assumed $f'(x^*) = 0, \, f''(x^*) < 0$ and $f(x) > 0$. Hence, $x^*$
also maximize $\log(f(x))$.

Thus, taking $\beta$ to maximize the log-likelihood is equivalent to maximizing the likelihood itself. Finally, we take our 
cost function to be the so-called \textit{cross-entropy} which is defined as the negative log-likelihood 
\begin{equation}
 C(\beta) \equiv  -l(\beta).
\end{equation}
Then, $\beta$ is found by \textit{minimizing} the cross-entropy. 

Note here that we can not find a analytical solution for the maximizer. This means that we have to use a
numerical opimization algorithm, such as gradient descent which we discuss later, to find the optimal parameters.

However, the gradient of the cross entropy can be given in closed-form 
\begin{equation}
 \nabla_\beta C(\beta) = - X^{T} \left( \mathbf{y} - \mathbf{p} \right). 
\end{equation}
Here we have defined \begin{align}
\mathbf{y} &\equiv (y_1,\dots,y_n), \\       
      \mathbf{p} &\equiv \left( p(y_1=1|\mathbf{x}^{(1)}),\dots,p(y_n=1|\mathbf{x}^{(n)}) \right)
      \end{align}
and $X \in \mathbb{R}^{n \times (p+1)}$ is the design-matrix containing $\mathbf{x}^{(i)}$ as its i-th row.


\subsection[Neural networks]{Neural networks \protect\footnote{This section follows chapter 7 of \cite{ledum2017computational} because I am lazy.}}
Artificial neural networks (sometimes just neural networks) are computational models with the ability to \textit{learn} from examples it is shown. The structure of the networks are inspired by biological networks constituting animal brains. Artificial neural networks fall under the category of machine learning\textemdash a subfield of artificial intelligence\textemdash and we will, in the following, expose the precise mechanism of the model learning. 

Such neural networks can be created in numerous ways, but we will focus exclusively on the most common architecture, namely \emph{multilayer perceptrons} (MLP). The MLP neural networks are built from \emph{layers} of connected \emph{neurons}. In the artificial network, an input value (possibly a vector) is fed into the network model and then propagated through the layers, being processed through each neuron in turn. We will deal only with \emph{feed forward} ANNs, meaning information always flows through the net in one direction only\textemdash essentially there are no loops. The entire ANN produces an output value (possibly a vector), which means we can think of it as a complicated function $\mathbb{R}^n\mapsto \mathbb{R}^m$. As we will see, it is possible to write down a closed form expression for this function and it is\textemdash crucially\textemdash possible to devise an efficient algorithm for calculating the gradient of the entire function w.r.t.\ any of the internal parameters.

\subsubsection{Neurons and layers}
A neuron is simply a model function for propagating information through the network. Inspired by biological neurons, the artificial neuron "fires" if it is stimulated by a sufficiently strong signal. The artificial neuron receives a vector of input values ${\bf p}$. If the neuron is part of the very first hidden layer (this will be expanded upon shortly), the input is simply the input value(s) to the NN. If one or more layers preceded the current one, ${\bf p}$ is a vector of outputs from the neurons in the previous layer.

\begin{figure*}
\begin{centering}
\begin{tikzpicture}[
node distance = 5mm and 18mm,
  start chain = going below,
  arro/.style = {-Latex},
bloque/.style = {text width=4ex, inner sep=2pt, align=right, on chain},
                        ]
% inputs
\foreach \i [count=\j] in {p_{1}, p_{2}, p_{3}, {\vdots}, p_{N}}
    \node[bloque] (in-\j) {$\i$};
% output
\node (out) [circle, draw, minimum size=6mm,
      %label=\textsc{$\displaystyle\sum_{i=1}^N$},
      right=of $(in-3)!0.5!(in-3)$]  {$\displaystyle\sum_{i=1}^Nw_ip_i$};
% conections
\foreach \i in {1,...,3}
    \draw[arro] (in-\i) -- (out);
\draw[arro] (in-5) -- (out);
% output
\node (output) [circle, draw, minimum size=6mm,right=of out]   
    {$f\left[{\bf w}^T{\bf p} + b\right]$};
%\coordinate[right=of out] (output);
\draw[arro] (out) -- (output);
% layer labels
\node[above=of in-1.center]     {Input};
\node[below=of in-4 -| out]     (bias) [circle,draw,minimum size=6mm]
                    {$b$};
\draw[arro] (bias) -- (output);
\node[above=of in-1 -| output]  {Activation};
\node[above=of in-1 -| out]  {Sum};
\node[right=of output] (final) {$\tilde p$};
\node[above=of in-1 -| final]  {Output};
\draw[arro] (output) -- (final);
\end{tikzpicture}
\caption{A model neuron, a constituent part of the artificial neural network model. The input from the previous layer ${\bf p}$ multiplied by corresponding weights ${\bf w}$ and summed. Then the bias $b$ is added, and the activation function $f$ is applied to the resulting ${\bf w}^T{\bf p}+b$. The output $\tilde p$ goes on to become input for neurons in the next layer. \label{fig:neuron}}
\end{centering}
\end{figure*}
The neuron is connected to the previous layers' neurons, and the strength of the connection is represented by a vector of weights, ${\bf w}$. Let us now consider a neuron which we will label by the index $k$. The output from neuron $i$ (of the preceding layer), $p_i$, is multiplied by the weight corresponding to the $i$\----$k$ connection, $w_i$. The combined weight vector multiplied by the input vector gives part of the total activation of the neuron, 
\begin{align}
\sum_{i=1}^Nw_ip_i = {\bf w}^T{\bf p}.
\end{align}
The remaining part is known as the bias, $b_k$. This is a single real number. There is one for each neuron, and it acts as modifier making the neuron more or less likely to fire independently of the input. 

The total input is passed to an activation (or transfer) function, which transforms it in some specified way, yielding the neuron \emph{output} $\hat p_k$. This in turn becomes input for the neurons in subsequent layers. 

Various different activation functions $f$ are used for different purposes. The function may be linear or non-linear, but should vanish for small inputs and \emph{saturate} for large inputs. For reasons that will become clear shortly, the conditions we enforce on $f$ is continuity, boundedness, as well as non-constantness. We also demand it be monotonically increasing. Numerous different transfer functions are in popular use today, and we will outline some of them in section \ref{sect:activations}.

In total, the action of a single neuron can be written
\begin{align}
\text{input}\ \rightarrow f\left({\bf w}^T{\bf p}+b\right) = \tilde p \ \rightarrow \ \text{output}.
\end{align}
A schematic representation of the single neuron connected to the previous and acting as input for the next layers is shown in \fig{neuron}. 

The full artificial neural network is built up of layers of neurons. Data is fed sequentially through the network, starting in the input layer (the input values can be thought of as the first layer), through the \emph{hidden} layers, and ending up in the output layer. The propagation needs to happen simultaneously across the network, as layer $k$ needs the fully computed output of layer $k-1$ before the activations can be calculated. 

A layer is\textemdash put simply\textemdash a collection of neurons, all of which are connected to the previous layer's neurons and the next layer's neurons. Let us label the individual neurons in layer $k$ by index $i$, i.e.\ $n_i^k$. The bias of neuron $i$ is then denoted $b^k_i$, and the weights connecting $n_i^{k-1}$ to $n_j^k$ is called $w_{ji}$. For each neuron there is a corresponding weight, so the weight vector is denoted $\bm{w}_i^k$. The combination of all weight vectors for layer $k$ thus makes a matrix, which we will denote by a capital $W^k$,
\begin{align}
W^k=\pmat{ccccc}{
  w^k_{11} & w^k_{12} & w^k_{13} & \dots & w^k_{1N} \\
  w^k_{21} & w^k_{22} & w^k_{23} & \dots & w^k_{2N} \\
  w^k_{31} & w^k_{32} & w^k_{33} & \dots & w^k_{3N} \\
  \vdots  &  \vdots   &   \vdots   & \ddots & \vdots \\
  w^k_{N1} & w^k_{N2} & w^k_{N3} & \dots & w^k_{NN}
}, \nonumber
\end{align}
or more compactly $(W^k)_{ij}=w^k_{ij}$. The collection of all biases for layer $k$ is $\mathbf{b}^k$. In this notation, we may write the propagation of the signal from layer $k-1$ to layer $k$ as 
\begin{strip}
\begin{align}
\mathbf{y}^k &= f(W^k\mathbf{y}^{k-1}+\mathbf{b}^k) 
=f\left( \bmat{ccccc}{
  w^k_{11} & w^k_{12}  & \dots & w^k_{1N} \\
  w^k_{21} & w^k_{22}  & \dots & w^k_{2N} \\
  \vdots  &  \vdots      & \ddots & \vdots \\
  w^k_{N1} & w^k_{N2}  & \dots & w^k_{NN}
} 
 \bmat{c}{y^{k-1}_1\\ y^{k-1}_2  \\ \vdots \\ y^{k-1}_N}  
+\bmat{c}{b^{k}_1  \\ b^{k}_2    \\ \vdots \\ y^{k}_N} \right) \label{eq:nnmatrix}
\end{align}
\end{strip}
or in Einstein notation (no sum over $k$ implied)
\begin{align}
y^k_i= f\big(w^k_{ij}y^{k-1}_j+b^k_i  \big).
\end{align}
In all of the preceeding three equations, application of $f$ indicates \emph{element wise} functional evaluation.

It is clear from \eq{nnmatrix} that propagation through an entire layer can be thought of as a matrix-vector product, a vector-vector summation, and a subsequent application of the transfer function $f$ element-wise on the resulting vector. 

A schematic representation of a layer consisting of three artificial neurons in a fully connected ANN is shown in \fig{layer}.

%Raff s. 51: "NN (and other non-linear black box techniques) cannot be expected to \emph{extrapolate} accurately."

\begin{figure*}
\begin{centering}
\def\layersep{4.5cm}
\begin{tikzpicture}[shorten >=1pt,->,draw=black!50, node distance=\layersep]
    \tikzstyle{every pin edge}=[<-,shorten <=1pt]
    \tikzstyle{neuron}=[circle,minimum size=17pt,inner sep=0pt]
    \tikzstyle{input neuron} =[neuron, draw=gray];
    \tikzstyle{output neuron}=[neuron, draw=gray];
    \tikzstyle{hidden neuron}=[neuron, fill=black];
    \tikzstyle{annot} = [text width=4em, text centered]

    % Draw the input layer nodes
    \foreach \name / \y in {1,...,3}
    % This is the same as writing \foreach \name / \y in {1/1,2/2,3/3,4/4}
        \node[input neuron, pin=left:{}] (I-\name) at (0,-\y) {};

    % Draw the hidden layer nodes
    \foreach \name / \y in {1,...,3}
        \path[yshift=0.0cm]
            node[hidden neuron] (H-\name) at (\layersep,-\y cm) {};

    % Draw the output layer node
  %\foreach \name / \y in {1,...,3}
    %    \path[yshift=0.0cm]
    %        node[hidden neuron] (H-\name) at (-\layersep,-\y cm) {};
    \node[output neuron,pin={[pin edge={->}]right:{}}, right of=H-3] (O) {};
    \node[output neuron,pin={[pin edge={->}]right:{}}, right of=H-3] at (\layersep,-1) (OO) {};
    \node[output neuron,pin={[pin edge={->}]right:{}}, right of=H-3] at (\layersep,-2) (OOO) {};

    
    % Connect every node in the input layer with every node in the
    % hidden layer.
    \foreach \source in {1,...,3}
        \foreach \dest in {1,...,3} 
            \path (I-\source) edge (H-\dest);
            

    % Connect every node in the hidden layer with the output layer
    \foreach \source in {1,...,3} {
        \path (H-\source) edge (O);
    \path (H-\source) edge (OO);
    \path (H-\source) edge (OOO);
    }

    % Annotate the layers
    \node[annot,above of=H-1, node distance=1cm] (hl) {Layer\ \ \ \ \ $k$};
    \node[annot,left of=hl]  {Layer\ \ \ $k-1$};
    \node[annot,right of=hl] {Layer\ \ \ $k+1$};
\end{tikzpicture}
\end{centering}
\caption{Schematic representation of a single ANN layer. Each neuron of the layer indexed $k$ is connected from behind to all neurons in layer $k-1$. The connection weights can be organized into a matrix, $W^{k-1}$, and the action of layer $k$ can be succinctly stated as $f(W^{k}{\bf p}^{k-1}+{\bf b}^k)$ where element-wise operation is assumed for the activation $f$. \label{fig:layer}}
\end{figure*}

\subsubsection{The full network}
A collection of $L$ layers connected to eachother forms a full \emph{network}. Note carefully that the network is nothing more than a (somewhat complex) function. If a single input and a single output value is specified, the action of the NN can be written out in closed form as 
\begin{strip}
\begin{align}
&\hat y\! = \!\left.\!\sum_{j=1}^{M}\! w_{1j}^L f_L\!\left(\!\sum_{k=1}^{M}\! w_{jk}^{L-1} f_{L-1}\!\left(\!
\sum_{i=1}^Mw_{ki}^{L-2} f_{L-2}\left( \dots f_1\!\Big(\!w_{m1}^1 x_1 \! + \!b_m^1\!\Big)
 \!\dots \!\right)+ b_i^{L-2}\right) \!+ \!b_k^{L-1}\!\right)
 \!+ \!b_1^L\!\right.. \label{eq:nnfull}
\end{align}
\end{strip}
Here, we have taken each layer to consist of $M$ neurons. The scalar $x_1$ denotes the input value, while $\hat y$ is the NN output. The transfer functions (which are \textit{not} assumed to all be the same) are denoted $f_L, f_{L-1},\dots,f_1$. From looking at \eq{nnfull}, the usefulness of the model is in no way obvious. But it turns out that for an ANN with at least one hidden layer populated with a finite number of neurons is a \emph{universal approximator} \autocite{HORNIK1989359}. This holds under the aforementioned assumptions on $f$. Being a universal approximator means (in this context) that the NN function can be made to be arbitrarily close to any continuous Borel-measurable function (essentially \emph{any} function we are likely to encounter) \autocite{mcdonald}. 

\subsection{Activation functions \label{sect:activations}}
Without any transfer functions, i.e.\ $f_l(x)=x$ for all layers $l$, the full network would simply be a linear transformation. In order to introduce non-linearities in our model, we employ one (or more) of a large set of possible activations. As mentioned, we require that these functions are continous and at least once (almost everywhere\footnote{A function with a property \textit{almost everywhere} (a.e.) denotes a function which satisfies this property everywhere, \textbf{except} possibly on a set of measure zero (such as e.g.\ a single point).}) differentiable, in order for the backpropagation scheme of section \ref{sect:backprop} to work. 

The following is an incomplete outline of activation functions in common use today. The simplest possible activation, the identity transformation $f_\text{I}(x)=x$, is commonly used for the output layer in regression networks. A simple (Heaviside) step function, $f_\text{H}(x)=H(x)$, with 
\begin{align}
f_\text{H}(x)=
\begin{cases}
1 & \text{ if } x\ge0 \\
0 & \text{ if } x<0 
\end{cases},
\end{align}
is sometimes used in the output layer of classification networks, but seldom in hidden layers because of the vanishing gradient making it impossible to train with backpropagation. The sigmoid function, 
\begin{align}
f_\text{S}(x)=\frac{1}{1+\mathrm{e}^{-x}},
\end{align}
is commonly used as hidden layer activations along with its sibling the hyperbolic tangent
\begin{align}
f_\text{t}(x)=\tanh(x)=\frac{\mathrm{e}^{x}-\mathrm{e}^{-x}}{\mathrm{e}^{x}+\mathrm{e}^{-x}}.
\end{align}
The tangent activation is a simple rescaling of the sigmoid, with $f_\text{t}(x)=2f_\text{s}(2x)-1$.

Simpler than the sigmoid, the family of activations known as \textit{rectifiers} consist of piecewise linear functions which are popular nowadays. The basic variant, the rectified linear unit (ReLU) is defined as 
\begin{align}
f_\text{ReLU}(x)=\max(0,x),
\end{align}
and is popular mostly because of its application to training \textit{deep} (many, large layers) neural networks\autocite{glorot2011deep}. Many variants of the ReLU exist, among the most well known are the leaky ReLU,
\begin{align}
f_\text{leaky ReLU}(x;\alpha)=
\begin{cases}
x & \text{ if }x\ge0 \\
\alpha x & \text{ if } x<0,
\end{cases}
\end{align}
the noisy ReLU
\begin{align}
f_\text{NReLU}(x) = \max(0,x+\mathcal{N}(0,\sigma)),
\end{align}
and the exponential linear unit
\begin{align}
f_\text{ELU}(x;\alpha) = 
\begin{cases}
x & \text{ if } x\ge 0 \\
\alpha(\mathrm{e}^x-1) & \text{ if } x< 0.
\end{cases}
\end{align}

\subsection{Training neural networks}
Knowing that ANNs can be universal approximators is not helpful unless we can find a systematic way of obtaining suitable parameters to approximate any given function $g(x)$. This is where \emph{training} comes in. Teaching a NN to approximate a function is conceptually simple, and involves only three steps:
\begin{shadeframe}
Assume input $x$ and corresponding \emph{correct} output $y$ is known.
\begin{itemize}
  \item[(1)] Compute output $\text{NN}(x)=\hat y$ of the artificial neural network, and evaluate the \emph{cost} function, $C(\hat y,y)$. 
  \item[(2)] Compute the gradient of $C(\hat y)$ w.r.t.\ all the parameters of the network, $w_{ij}^k$ and $b^k_j$.  
  \item[(3)] Adjust the parameters according to the gradients, yielding a better estimate $\hat y$ of the true output value $y$. 
  \item[(4)] Repeat (1)\textemdash(4).
\end{itemize}
\end{shadeframe}
The training scheme is known as \emph{supervised learning}, because the NN is continually presented with $x$, $y$ pairs, i.e.\ an input and an expected output. The cost (or objective or loss) function determines how happy the network is with it's own performance. In general, the output of the neural network is a vector of values, $\mathbf{y}$, and the cost function is taken across all outputs. In \eq{cost}, the network produces $N_\text{O}$ outputs for each input (which itself may be a vector). 

Step (3) is easy to understand, but complex in practice. In order to update the network weights and biases, a measure of the expected change in the total output is needed. Otherwise, any change would just be done at random\footnote{This is a possible approach, yielding a class of \emph{genetic} optimization algorithms. We will not discuss such schemes in the present work.}. This means we need to compute the set of derivatives 
\begin{align}
g_{ij}^k \equiv \frac{\partial C(\hat{\mathbf{y}})}{\partial w_{ij}^k}, \ \ \ \text{ and } \ \ \ h_i^k\equiv \frac{\partial C(\hat{\mathbf{y}})}{\partial b^k_i}.
\end{align}
The most common algorithm for computing these derivatives is the {\bf backpropagation} algorithm \autocite{backprop}. The method works by first pushing an input through the ANN, and computing the derivatives of the cost function w.r.t.\ the last layer weights and biases. The network is then traversed backwards, and the gradient w.r.t.\ all neuron parameters is found by repeated application of the chain rule. 

\subsection{Backpropagation \label{sect:backprop}}
Before we can apply the backpropagation algorithm, we need to perform a forward pass of the network given some input vector $\mathbf{x}\in\mathds{R}^{n_\text{f}}$, where $n_\text{f}$ denotes the number of features in the input data. We consider\textemdash for the moment\textemdash the case of a single input only ($n_\text{inputs}=1$). During the forward pass we calculate the activations $a^l$ of layer $l$, i.e.\ 
\begin{align}
a^l_j = f_l(z_j^l), 
\end{align}
where $z_j^l$ is the sum of a weighted sum of inputs from the previous layer and the bias of layer $l$, 
\begin{align}
z_j^1 = \sum_{i=1}^{n_\text{f}}w_{ij}^1x_i+b_j^1.
\end{align}
Assuming the layers have $N_l$ number of neurons, the calculated $z_j^l$ of subsequent layers takes the form
\begin{align}
z_j^l = \sum_{i=1}^{N_{l-1}}w_{ij}x_i+b_j^l,
\end{align}
where we note that $W^l\in\mathds{R}^{N_{l-1}\times N_l}$.

After performing the forward pass, we calculate the cost function and its derivative w.r.t.\ the weights in the output layer $W^L$,
\begin{align}
\frac{\partial C(W^L)}{\partial w_{jk}^L} &= \frac{\partial C(W^L)}{\partial a_j^L}\left[\frac{\partial a_j^L}{\partial w_{jk}^L}\right] \nonumber\\
%
&= \frac{\partial C(W^L)}{\partial a_j^L} \left[ \frac{\partial a_j^L}{\partial z_j^L}\frac{\partial z_j^L}{\partial w_{jk}^L} \right] \nonumber \\
%
&= \frac{\partial C(W^L)}{\partial a_j^L} f_L'(z_j^L)  a_k^{L-1}, \label{eq:deltaL}
\end{align}
where we used that (note $y_j=a_j^L$, i.e.\ the activations of the final layer)
\begin{align}
\frac{\partial C(W^L)}{\partial a_j^L} &= \frac{\partial }{\partial a_j^L} \left[\frac{1}{2}\sum_{i=1}^{N_\text{O}} (a_j^L-t_j)^2\right] \nonumber \\
&= a_j^L-t_j,
\end{align}
and 
\begin{align}
\frac{\partial z_j^L}{\partial w_{jk}^L} &= \frac{\partial}{\partial w_{jk}^L} \left[\sum_{p=1}^{N_L} w_{jp}^La_j^{L-1}+b_j^L\right] \nonumber \\
&= a_j^{L-1}. 
\end{align}

We define the quantity in \eq{deltaL} (apart from $a_k^{L-1}$ as $\delta_j^L$, meaning $\partial C/\partial w_{jk}^L=\delta^L_ja_k^{L-1}$. Applying the chain rule to $\delta^L_j$ yields the derivative of the cost function w.r.t.\ the output layer biases as
\begin{align}
\delta^L_j&= \frac{\partial C(W^L)}{a_j^L}\frac{\partial f^L}{\partial z_j^L} = \frac{\partial C(W^L)}{a_j^L}\frac{\partial a_j^L}{\partial z_j^L} \nonumber \\
%
&= \frac{\partial C(W^L)}{\partial z_j^L} = \frac{\partial C(W^L)}{\partial b_j^L} \frac{\partial b_j^L}{\partial z_j^L} \label{eq:deltad} \\
&= \frac{\partial C(W^L)}{\partial b_j^L},
\end{align}
where we used that 
\begin{align}
\frac{\partial b_j^L}{\partial z_j^L} &= \left[\frac{\partial z_j^L}{\partial b_j^L}\right]^{-1} \nonumber \\
%
&= \left[\frac{\partial}{\partial b_j^L} \sum_{i=1}^{N_{L-1}} w_{ij}^L a_i^{L-1}+b_j^L \right]^{-1} \nonumber \\
%
&= 1.
\end{align}
We have thus found the derivatives of the cost function w.r.t.\ both the weights and biases in the output layer, $W^L$ and $\mathbf{b}^L$. 

The equation 
\begin{align}
\delta_j^l &= \frac{\partial C}{\partial z_j^l}
\end{align}
holds for any layer, not just the output as in \eq{deltad}. Relating this to derivatives w.r.t.\ the layer $l+1$ $z_j$s, we find
\begin{align}
\delta_j^l &= \frac{\partial C}{\partial z_j^l} = \sum_k\frac{\partial C}{\partial z_k^{l+1}} \frac{\partial z_k^{l+1}}{z_j^l} \nonumber \\
&= \sum_k \delta_k^{l+1}\frac{\partial z_k^{l+1}}{z_j^l} \nonumber \\
%
&= \sum_k \delta_k^{l+1}w_{kj}^{l+1}\frac{\partial f^l}{\partial z_j^l}, \label{eq:backpropeq}
\end{align}
with 
\begin{align}
\frac{\partial z_k^{l+1}}{\partial z_j^l} &= \frac{\partial}{\partial z_j^l}\left[\sum_{i=1}^{N_l}w_{ik}^{l+1}a_k^l + b_k^{l+1}\right] \nonumber \\
%
&= \frac{\partial}{\partial z_j^l}\left[\sum_{i=1}^{N_l}w_{ik}^{l+1}f^l(z_k^l) + b_k^{l+1}\right] \nonumber \\
&= w_{jk}^{l+1}f^l(z_j^l).
\end{align}
The rest of the backpropagation scheme is essentially iterating \eq{backpropeq}, and computing\textemdash for each layer\textemdash the gradients $\partial C/\partial w_{ij}^l=\delta_i^la_j^{l-1}$ and $\partial C/\partial b_i^l=\delta_i^l$. Once the gradients are known, updating the weights and biases to improve the performance of the network (making the cost function smaller) can be done by e.g.\ gradient descent schemes, c.f.\ section \ref{sect:gradientdescent}.

\subsection{Exploding / vanishing gradients and weight initialization}
Typical transfer functions are constant or close to constant on most of $\mathds{R}$, and only changes appreciably in a tiny region around the origin. This means that a fully \textit{saturated} neuron with input $z_j\gg1$, or a \textit{dead} neuron with input $z_j\ll-1$ will most likely exhibit very small gradients and change very little durign training. This means the neurons are essentially wasted, they only add a constant input to the neurons in the subsequent layer; a job already performed by the bias $b_{j+1}$. In order to avoid this effect, it is important to initialize the weight matrices in the network in a smart way. 

In the useful region around the origin, we may assume that the transfer functions are roughly linear. In order for the signal to propagate through the network usefully, we essentially want the mean value of the $z_j$s to vanish, and the variance to be on the order of $1$. Let us now consider an input vector $X$ of $n$ components. If we take the weights to be random\textemdash as is the case in the first forward pass\textemdash then $W$ is a random vector of weights $W_i$. With the previous assumption about linearity of the transfer function, we get the activation 
\begin{align}
A=W_1X_1+W_2X_2+\dots+W_nX_n,
\end{align}
which has variance 
\begin{align}
\text{Var}(A) &= \text{Var}(W_1X_2+\dots+W_nX_n) \nonumber \\
%
&= n\text{Var}(W_i)\text{Var}(X_i),
\end{align}
where we assumed that the inputs and the weights are all independent, identically distributed, with vanishing mean. If we want the activation to have variance on the order of $1$, then we must require that the variance of the weights is 
\begin{align}
\text{Var}(W_i) = \frac{1}{n}.
\end{align}
Performing the same analysis with the backpropagated signal yields the same result,
\begin{align}
\text{Var}(W_i) = \frac{1}{n'},
\end{align}
with $n'$ being the amount of weights in the \textit{next} network layer. 

With this in mind,  Glorot \& Bengio suggested initializing weights with average variance:\autocite{glorot2010understanding}
\begin{align}
\text{Var}(W_i^l) = \frac{1}{n_l+n_{l+1}}.
\end{align}
In the case of sigmoid or hyperbolic tangent transfer functions, we may realize this variance by initializing $w=\mathcal{U}(-r,r)$ with
\begin{align}
r_\text{sigmoid}=\sqrt{\frac{6}{n_\text{in}+n_\text{out}}}, \nonumber \\
r_\text{tanh}=4\sqrt{\frac{6}{n_\text{in}+n_\text{out}}}, \nonumber
\end{align}
where $\mathcal{U}(a,b)$ denotes a uniform distribution between $a$ and $b$ and $n_\text{in}$ ($n_\text{out}$) is the number of neurons in the current (next) layer.

With rectifying linear units, the weight initialization scheme changes slightly. As the ReLU transfer function vanishes across half of $\mathds{R}$, He et al.\autocite{he2015delving} suggest doubling the variance of the weights in order to keep the propagating signal's variance constant, i.e.\
\begin{align}
\text{Var}(W)=\frac{2}{n_\text{in}}.
\end{align}
We may realize this by initializing weights $w=\mathcal{N}(0,1)\sqrt{2/n_\text{in}}$, where $\mathcal{N}(\mu,\sigma)$ denotes the normal distribution with mean $\mu$ and standard deviation $\sigma$.



\subsection{Gradient Descent \label{sect:gradientdescent}}
Almost every problem in machine learning and data science starts
with a dataset $X$, a model $g(\theta)$, which is a function of the parameters $\theta$ and a cost 
function $C(X, g(\theta))$ that allows us to judge how well the
model $g(\theta)$ explains the observations $X$. The model is fit by finding the values of $\theta$ that minimize the 
cost function. Ideally we would be able to solve for $\theta$ analytically, however this is not possible in general and 
we must use numerical methods to compute the minimum.
\subsubsection{The method of steepest descent}
The basic idea of gradient descent is that a function $F(\mathbf{x})$, $ \mathbf{x} \equiv (x_1,\cdots,x_n)$, 
decreases fastest if one goes from $\bf {x}$ in the direction of the negative gradient $-\nabla F(\mathbf{x})$.
It can be shown that if 
\begin{equation}
\mathbf{x}_{k+1} = \mathbf{x}_k - \gamma_k \nabla F(\mathbf{x}_k), \ \ \gamma_k > 0
\end{equation}
for $\gamma_k$ small enough, then $F(\mathbf{x}_{k+1}) \leq F(\mathbf{x}_k)$. This means that for a sufficiently 
small $\gamma_k$ we are always moving towards smaller function values, i.e a minimum. 

This observation is the basis of the method of steepest descent, which is also referred to as just gradient descent (GD). 
One starts with an initial guess $\mathbf{x}_0$ for a minimum of $F$ and compute new approximations according to
\begin{equation}
\mathbf{x}_{k+1} = \mathbf{x}_k - \gamma_k \nabla F(\mathbf{x}_k), \ \ k \geq 0.
\end{equation}
The parameter $\gamma_k$ is often referred to as the step length or the learning rate in the context of ML.

Ideally the sequence $\{ \mathbf{x}_k \}_{k=0}$ converges to a \textit{global} minimum of the function $F$. 
In general we do not know if we are in a global or local minimum.  
In the special case when $F$ is a \textit{convex function, all local minima are also global minima}, so in this case gradient 
descent can converge to the global solution. The advantage of this scheme is that it is conceptually simple and 
straightforward to implement. 

However the method in this form has some severe limitations:

\begin{itemize}
 \item In machine learing we are often faced with non-convex high dimensional cost functions with many local minimum. 
 Since GD is deterministic we will get stuck in a local minimum, if the method converges, unless we have a very good intial 
 guess. This also implies that the scheme is sensitive to the chosen initial condition.
 \item Note that gradient is a function of $\mathbf{x} = (x_1,\cdots,x_n)$ which makes it expensive to compute numerically.
 \item GD is sensitive to the choice of learning rate $\gamma_k$. This is due to the fact that we are only guaranteed 
that $F(\mathbf{x}_{k+1}) \leq F(\mathbf{x}_k)$ for \textit{sufficiently} small $\gamma_k$. The problem is to determine an 
optimal learning rate. If the learning rate is chosen to small the method will take a long to converge and if it is to 
large we can experience erratic behavior.
\item Many of these shortcomings can be alleviated by introducing randomness. One such method is that of Stochastic Gradient Descent 
(SGD).
\end{itemize}

\subsubsection{Stochastic Gradient Descent \label{sect:stochasticgradient}}
Stochastic gradient descent (SGD) and variants thereof address some of the shortcomings of the Gradient descent method discussed above. 

The underlying idea of SGD comes from the observation that the cost function, which we want to minimize, can almost always be 
written as a sum over $n$ datapoints $\{\mathbf{x}_i\}_{i=1}^n$,
\begin{equation}
 C(\mathbf{\theta}) = \sum_{i=1}^n c_i(\mathbf{x}_i, \mathbf{\theta}). 
\end{equation}
This in turn means that the gradient can be computed as a sum over $i$-gradients 
\begin{equation}
\nabla_\theta C(\mathbf{\theta}) = \sum_i^n \nabla_\theta c_i(\mathbf{x}_i, \mathbf{\theta}).  
\end{equation}
Now, stochasticity/randomness is introduced by only taking the gradient on a subset of the data called minibatches.  
If there are $n$ datapoints and the size of each minibatch is $M$, there will be $n/M$ minibatches. We denote these 
minibatches by $B_k$ where $k=1,\cdots,n/M$. 

As an example, suppose we have $10$ datapoints $( \mathbf{x}_1, \cdots, \mathbf{x}_{10} )$ and we choose to have $M=5$
minibathces, then each minibatch contains two datapoints. In particular we have 
$B_1 = (\mathbf{x}_1,\mathbf{x}_2), \cdots, B_5 = (\mathbf{x}_9,\mathbf{x}_{10})$. Note that if you choose $M=1$ you have 
only a single batch with all datapoints and on the other extreme, you may choose $M=n$ resulting in a minibatch for each 
datapoint, i.e $B_k = \mathbf{x}_k$.

The idea is now to approximate the gradient by replacing the sum over all datapoints with a sum over the datapoints in one the 
minibatches picked at random in each gradient descent step
\begin{align}\nabla_\theta C(\mathbf{\theta}) = &\sum_{i=1}^n \nabla_\theta c_i(\mathbf{x}_i, \mathbf{\theta}) \nonumber \\
\rightarrow &\sum_{i \in B_k}^n \nabla_\theta c_i(\mathbf{x}_i, \mathbf{\theta}).
\end{align}
                                                                                                                                                                                                 
Thus a gradient descent step now looks like 
\begin{equation} \theta_{j+1} = \theta_j - \gamma_j \sum_{i \in B_k}^n \nabla_\theta c_i(\mathbf{x}_i, \mathbf{\theta}) \end{equation}
where $k$ is picked at random with equal probability from the interval $[1,n/M]$. An iteration over the number of 
minibathces $n/M$ is commonly referred to as an epoch. Thus it is typical to choose a number of epochs and for each epoch 
iterate over the number of minibatches.
 
Taking the gradient only on a subset of the data has two important benefits. First, it introduces randomness 
which decreases the chance that our opmization scheme gets stuck in a local minima. Second, if the size of the 
minibatches are small relative to the number of datapoints ($M < n$), the computation of the gradient is much cheaper 
since we sum over the datapoints in the k-th minibatch and not all $n$ datapoints. 

A natural question is when do we stop the search for a new minimum? One possibility is to compute the full gradient after a 
given number of epochs and check if the norm of the gradient is smaller than some threshold and stop if true. However, 
the condition that the gradient is zero is valid also for local minima, so this would only tell us that we are close to a 
local/global minimum. However, we could also evaluate the cost function at this point, store the result and continue 
the search. If the test kicks in at a later stage we can compare the values of the cost function and keep the $\theta$ that 
gave the lowest value. 

Another approach is to let the step length $\gamma_j$ depend on the number of epochs in such a way that it becomes very small 
after a reasonable time such that we do not move at all. 

As an example, let $e = 0,1,2,3,\cdots$ denote the current epoch and let $t_0, t_1 > 0$ be two fixed numbers. Furthermore, 
let $t = e \cdot m + i$ where $m$ is the number of minibatches and $i=0,\cdots,m-1$. Then the function 
\begin{equation}\gamma_j(t; t_0, t_1) = \frac{t_0}{t+t_1} \end{equation}
goes to zero as the number of epochs gets large. I.e. we start with a step length $\gamma_j (0; t_0, t_1) = t_0/t_1$ which 
decays in "time" t. 

In this way we can fix the number of epochs, compute $\theta$ and evaluate the cost function at the end. Repeating the 
computation will give a different result since the scheme is random by design. Then we pick the final $\theta$ that gives 
the lowest value of the cost function.

\subsubsection{Gradient descent improvements: Adam}
While the stochastic gradient descent alleviates some of the problems intrinsic in the basic steepest descent method, it still has some problems. The \textit{stochasticity} allows it to possibly make jumps over small barriers, essentially transitioning into other basins of more optimal local minima. However, the SGD method struggles with navigating surfaces in parameter space in which the gradient is much steeper in one direction than the other(s). In this case, the iterations $\theta_i=(\alpha_i,\beta_i)$ will rapidly oscillate between over/under-shooting $\alpha_i$ values (with the steep gradient), while slowly making progress towards the minimum in $\beta_i$ (the shallow gradient).

We can help the SGD overcome this problem by introducing a \textit{momentum} term \autocite{qian1999monentum}. Instead of recomputing the gradient at each iteration, we keep a part of the change at the previous time step, essentially giving the optimization a momentum\textemdash accelerating the minimization in \textit{parameter space directions} in which the gradient is not steep, but consistently has a small value aimed steadily in one direction. It also hampers the rapid oscillating solutions in \textit{parameter space directions} in which we are close to the optimium, and the steep gradient makes the SGD over/under-shoot the solution at every other iteration. 

Each minibatch, the parameter update changes to
\begin{align}
\theta_{j+1} =\theta_j - \left[\eta \nabla_\theta c_i(\mathbf{x}_i,\theta_{j-1}) \right] - \gamma_j\nabla_\theta c_i(\mathbf{x}_i,\theta_j), \nonumber
\end{align}
with the momentum term $\eta$ usually set to a value close to $1.0$, e.g.\ $\eta=0.9$. The momentum term may be extended with a \textit{Nesterov accelerated gradient} scheme, which basically adds adaptive momentum term.

While introducing momentum into the SGD method improves the scheme, we are still disregarding a lot of previous\textemdash possibly relevant\textemdash information when we re-compute the gradient at each iteration and throw away all the history of previous gradients computed. In general, we should be able to use past moments of the calculated in previous iteration steps as a guide for the current gradient step in order to improve performance. This is exactly the motivation behind the Adam (Adaptive moment estimation) optimizer\autocite{kingma2014adam}.

\begin{algocf*}
\begin{shadeframe}
\begin{itemize}
  \item[] At step $j=0$, initialize $m_0=v_0=0$.
  \item[(1)] Iterate the step counter $j\leftarrow j+1$.
  \item[(2)] Calculate the gradient $g_j\leftarrow \nabla_\theta c(\mathbf{x}_i,\theta_j)$.
  \item[(3)] Update biased first moment estimate, \phantom{d}  $m_j\leftarrow \beta_1m_{j-1}+(1-\beta_1)g_j$.
  \item[(4)] Update biased second moment estimate, $v_j\leftarrow \beta_2v_{j-1}\phantom{d}+(1-\beta_2)g_j^2$.
  \item[(5)] Compute the bias-corrected first moment esimate, \phantom{d} $\hat m_j\leftarrow \frac{m_j}{1-\beta_1^j}$.
  \item[(6)] Compute the bias-corrected second moment esimate, $\hat v_j  \leftarrow \frac{v_j}{1-\beta_2^j}$.
  \item[(7)] Update parameter vector, $\theta_j\leftarrow \theta_{j-1} - \alpha \frac{\hat m_j}{\sqrt{\hat v_j} + \varepsilon}$.
\end{itemize}
\end{shadeframe}
\captionof{algocf}{The \textit{Adam} optimizer for stochastic optimization. The constants $\beta_1$, $\beta_2$, and $\varepsilon$ take appropriate default values $\beta_1=0.9$, $\beta_2=0.999$, and $\varepsilon=10^{-8}$. The initial step size $\alpha$ may be taken to be $0.001$. The gradient squared, $g_j^2$, denotes the elementwise square. \label{alg:adam}}
\end{algocf*}

The Adam scheme uses a moving, exponentially decaying, average of the first and second moments of the gradient to compute individual adaptive learing rates for each parameter independently. The moving average of the gradient $m_j$ is an estimate of the mean of the gradient, while the moving average of the gradient squared $v_j$ is an estimate of the (uncentered\footnote{The uncentered variance estimate of $\{f_i\}_{i=1}^N$, $\text{Var}_\text{u}(f)$ is the variance computed assuming $\langle f_i\rangle =0$, meaning the average is \textit{not} subtracted for each sample like usual,
\begin{align}
\text{Var}_\text{u}(f)&= \text{E}\left[\Big( f \Big)^2\right] \not= \text{E}\left[\Big( f - \langle f\rangle \Big)^2\right] = \text{Var}(f). 
\end{align}}) variance of the gradient. At the first iteration, we assign $m_0=v_0=0$, which means the estimates are inherently biased towards zero. In the Adam scheme, this is counteracted by computing bias-corrected estimates $\hat m_j$ and $\hat v_j$. The Adam optimization is outlined in algorithm \ref{alg:adam}.



\section{Model systems}
In this work we apply the machine learning algorithms discussed to the one- and two-dimensional Ising model. 
Linear regression and neural networks are used to estimate the coupling constant of the one-dimensional Ising model.

The two-dimensional Ising model is known to show phase transition. In particular, these phases can be labeled as ordered 
or disordered. Thus it is interesting to investigate whether logistic regression or neural networks can be trained to to 
classify the phases. 

\subsection{The one-dimensional Ising model}
\lipsum[6]
\subsection{The two-dimensional Ising model}
\lipsum[7]
\section{Results and discussion}
\lipsum[8]
\subsection{Learning the one-dimensional Ising Hamiltonian}
\lipsum[9]
\subsection{Classifying phases of the two-dimensional Ising model}
\lipsum[10]
\section{Conclusion}
\lipsum[11]

\onecolumn{
\printbibliography
}

\end{document}



