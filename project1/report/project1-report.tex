\documentclass[a4paper, twocolumn]{article}

% Included packages ---------------------------------------------------------- %
\usepackage{lipsum}                          % Generate random, blind, filler-text.
\usepackage[utf8]{inputenc}                  % utf-8 encoding, æ, ø , å, etc.
\usepackage{a4wide}                          % Adjust margins to better fit A4 format.
\usepackage{array}                           % Matrices.
\usepackage{dsfont}                          % Math symbols.
\usepackage{amsmath}                         % Math symbols, and enhanced matrices.
\usepackage{amsfonts}                        % Math fonts.
\usepackage{amssymb}                         % Additional symbols.
\usepackage{mathrsfs}                        % Most additional symbols.
\usepackage[pdftex]{graphicx}                % Improved inclusion of .pdf-graphics files.
\usepackage{sidecap}                         % Floats with captions to the right/left.
\usepackage{enumerate}                       % Change counters (arabic, roman, etc.).
\usepackage{floatrow}                        % Multi-figure floats.
\usepackage{subfig}                          % Multi-figure floats.
\usepackage{bm}                              % Bolded text in math mode.
\usepackage[framemethod=default]{mdframed}   % Make boxes.
\usepackage{listings}                        % For including source code.
\usepackage{mathtools}                       % Underbrackets, overbrackets.
\usepackage[dvipsnames]{xcolor}              % Colors.
\usepackage{capt-of}                         % Caption things which are not floats.
\usepackage{fontawesome}                     % Github icon, etc. \faGithub
\usepackage{sidecap}                         % Floats with captions on the side.
\usepackage{tabularx}                        % Tables and stuff.
\usepackage{tabulary}                        % Tables and stuff.
\usepackage[sf,sl,outermarks]{titlesec}      % Change fonts in section{}, subsection{}, etc.
\usepackage[subfigure]{tocloft}              % Change spacing between numbers and titles in TOC.
\usepackage{booktabs}                        % \toprule, \midrule, etc. for tables.
\usepackage{siunitx}                         % Allows S table column, aligning on decimal point.
\usepackage{chngcntr}                        % Change counter behaviour, supress increment of sub counters.
\usepackage[%                                % Adds functionality to captions.
  tableposition = top,
  labelsep      = period,
  justification = raggedright,
  format        = hang,
  ]{caption}                                 
\usepackage[%                                % Interactive references and links, colored.
  colorlinks  = true,
  linkcolor   = black,
  urlcolor    = blue,
  citecolor   = black,
  linktocpage = true,
  ]{hyperref}            
\usepackage[%                                % References, in super-script form.
  autocite    = superscript,
  backend     = biber,
  sortcites   = true,
  style       = numeric-comp,
  sorting     = none,
  url         = false,
  ]{biblatex}
\usepackage[autostyle, english = american]{csquotes} % Assure quotation marks are inserted correctly aligned left/right.
\MakeOuterQuote{"}

% Package settings ----------------------------------------------------------- %
\renewcommand{\thesection}{\Roman{section}}         % I, II, III, IV, etc. section numbering
\renewcommand{\thesubsection}{\Alph{subsection}}    % A, B, C, etc. subsection numbering
\renewcommand{\thesubsubsection}{}                  % Remove subsubsection numbering.
\floatsetup[table]{capposition=top}                 % Place table captions above the table.
\captionsetup[subfigure]{labelformat=empty}         % Remove the (a), (b), etc. tags from subfigures.
\advance\cftsecnumwidth 1.0em\relax                 % Set the spacing between section headings and titles in TOC with tocloft.
\advance\cftsubsecindent 1.0em\relax                % Set the spacing between subsection headings and titles in TOC with tocloft.
\advance\cftsubsecnumwidth 1.0em\relax              % Set the spacing between subsubsection headings and titles in TOC with tocloft.
\newcommand{\listingsfont}{\ttfamily}
\newcommand{\inlinepy}[1]{\lstinline[language={python}]{#1}}
\newcommand{\inlinecc}[1]{\lstinline[language={c++}]{#1}}
\counterwithout*{subsection}{section}               % Dont reset the subsection counter on new \section{} calls.
\renewcommand{\figurename}{FIG.}                    % Captions of figures read FIG.
\renewcommand{\tablename}{TABLE}                    % Captions of tables read TABLE 
\renewcommand{\thetable}{\Roman{table}}             % Number tables with roman numerals.

% Section headings settings -------------------------------------------------- %
\titleformat{\section}[hang]  % {command}[shape]
  {\normalfont\bfseries}      % {format}
  {\thesection.}              % {label}
  {2ex}                       % {sep}
  {\centering\MakeUppercase}  % {before-code}[after-code]

\titleformat{\subsection}[hang] % {command}[shape]
  {\normalfont\bfseries}        % {format}
  {\thesubsection.}             % {label}
  {1ex}                         % {sep}
  {\centering}                  % {before-code}[after-code]

\titleformat{\subsubsection}[hang]  % {command}[shape]
  {\normalfont\bfseries}            % {format}
  {}                                % {label}
  {1ex}                             % {sep}
  {\centering}                      % {before-code}[after-code]


% References ----------------------------------------------------------------- %
\newcommand{\Fig}[1]{Fig.\ \ref{fig:#1}}
\newcommand{\fig}[1]{Fig.\ \ref{fig:#1}}
\newcommand{\eq} [1]{Eq.\ (\ref{eq:#1})}
\newcommand{\Eq} [1]{Eq.\ (\ref{eq:#1})}
\newcommand{\tab}[1]{Table \ref{tab:#1}}
\newcommand{\Tab}[1]{Table \ref{tab:#1}}

% Matrices ------------------------------------------------------------------- %
\newcommand{\mat} [2]{\begin{matrix}[#1] #2 \end{matrix}}    % Nothing enclosing it.
\newcommand{\pmat}[2]{\begin{pmatrix}[#1] #2 \end{pmatrix}}  % Enclosing parentheses.
\newcommand{\bmat}[2]{\begin{bmatrix}[#1] #2 \end{bmatrix}}  % Enclosing square brackets.
\newcommand{\vmat}[2]{\begin{vmatrix}[#1] #2 \end{vmatrix}}  % Enclosing vertical bars.
\newcommand{\Vmat}[2]{\begin{Vmatrix}[#1] #2 \end{Vmatrix}}  % Enclosing double bars.

% Manually set alignment of rows / columns in matrices (mat, pmat, etc.) ----- %
\makeatletter
\renewcommand*\env@matrix[1][*\c@MaxMatrixCols c]{%
  \hskip -\arraycolsep
  \let\@ifnextchar\new@ifnextchar
  \array{#1}}
\makeatother

% figures in multicols environment ------------------------------------------- %
\newenvironment{Figure}
  {\par\medskip\noindent\minipage{\linewidth}}
  {\endminipage\par\medskip}

% Set bibliography file and path for images.
\addbibresource{../ref/project1-references.bib}
\bibliography{../ref/project1-references.bib}
\graphicspath{{../figures/}}

% Black frame with gray background ------------------------------------------ %
\definecolor{gray}{gray}{0.9}
\newmdenv[linecolor=white,backgroundcolor=gray]{grayframe}


% Title
\title{{\sc Regression analysis and resampling methods \\ {\large FYS-STK4155: Project 1}}}
\author{Morten Ledum \& Håkon Kristiansen \\ \faGithub \ {\small \href{https://github.com/mortele/FYS-STK4155/tree/master/project1}{github.com/mortele/FYS-STK4155}}}
% ---------------------------------------------------------------------------- %
% ---------------------------------------------------------------------------- %
\begin{document}


\twocolumn[
  \begin{@twocolumnfalse}
\maketitle

\begin{abstract}
We parameterize digital terrain data using linear regression analysis algorithms: Ordinary least squares (OLS), Ridge regression, and Lasso regression. The bootstrap resampling technique is used to gauge the bias and variance of the models. We use basis sets of homogeneous monomials in two variables, up to and including total degree 5. We find that the ordinary least squares scheme produces the best fit overall, with the Lasso regression method coming in a distand second. Visualizing the bias-variance tradeoff reveals that we are most likely in a region of bias dominated MSE, with the variance having essentially no impact on the mean error. In order for the Ridge and Lasso regularizations to become more effective than the OLS scheme, we postulate that much higher model complexities than those achieved currently are needed (polynomials on the order of $p>20$).

For initial validation of our models, we employ the test function of R.\ Franke\autocite{franke1979critical}.
\end{abstract}

\tableofcontents 
  \end{@twocolumnfalse}
]
\newpage

%\begin{multicols}{2}
\section{Introduction}
Linear regression methods are fundamental in the field of statistical modelling. In the simplest variant of regression analysis\textemdash modelling a response dependent on some predictor variables\textemdash a linear ansatz is assumed for the relasionship between response and predictors. The ansatz is linear in the sense of a linear combination of (possibly non-linear) functions of a single predictor. This linearity allows the matrix-vector equations resulting from minimizing the error between the observed data and the predicted model data to be swiftly solved, which is the main advantage of the method (and the main reason why linear regression is extensively used today). 

In the modern society, data is ubiquitous and abundant. However, with an ever increasing volume of data available, drawing sensible conclusions about the relasionships between variables is highly non-trivial. Suppose you were given a complete set of statistics about every school district in Norway: The grade point average (and indeed individual grades) of every single student, the size and composition (in terms of nationalities, gender, height, weight, etc.) of every class, a full description of the teaching load of every teacher, as well as their personal data, educational history, and work history. Such a data set might be readily available, but drawing conclusions from it is not easy. Say e.g.\ that you wanted to know if increasing the number of teachers per pupil would result in overall better grades. Or\textemdash a more complex questions\textemdash what is the most efficient way to increase the overall grades of students? 

Such questions pertaining to relationships between data of varying types are possible to answer using regression analysis, and bound on the statistical significance of the answers are possible to find. In the following, we will consider a much simpler toy problem as a simple introduction to the topic: Fitting a real valued function of two real variables. We will start out by considering the theory behind linear regression.

\section{Theory}
In the following we briefly introduce the theory underlying the technical aspects of the present work. We begin by considering linear regression in general, and the ordinary least squares (OLS) method.

\subsection{Linear regression}
In order to introduce the least squares methods, we consider a case in which $p$ characteristics of $n$ samples are measured. The outcome, or the \textit{response}, is denoted $\mathbf{y}$: a vector of size $n$. The measured characteristics, denoted the predictors, are organized in a matrix $\mathbf{X}$ of size $n\times p$. This is called the \textit{design matrix}.

In regression analysis, we aim to explain the response in terms of the predictors, i.e.\ construct a function $\mathbf{y}(\mathbf{X})$. Assuming a linear relationship between $\mathbf{X}$ and $\mathbf{y}$ gives rise to \textit{linear regression}, in which the response can be written as 
\begin{align}
\mathbf{y}=\mathbf{X}\bm{\beta}+\bm{\varepsilon},
\end{align}
where $\bm{\varepsilon}$ denotes the deviation of the linear model $\mathbf{X}\bm{\beta}$ and the response $\mathbf{y}$ and $\bm{\beta}$ is a parameter vector containing the linear regression coefficients $\beta_i$. The $\beta_i$ variables are the unknowns in the linear regression problem, and they represent the partial derivative of the \textit{modelled} response w.r.t.\ the descriptors. 

In any non-trivial case, the error terms $\varepsilon_i$ in the error vector $\bm\varepsilon$ will be non-zero. In this case, we regard our linear anzats as a \textit{model} of the true response, the observed values $y_i$. We denote our model by $\tilde{\mathbf{y}}$, and define 
\begin{align}
\tilde{\mathbf{y}} &= \mathbf{X}\bm{\beta} \\
%%
&= \mathbf{y}-\bm{\varepsilon}. \nonumber
\end{align}
The objective of linear regression thus emerges: Determine $\bm\beta$ in such a way that $\bm\varepsilon$ is minimized, thus giving a best possible linear fit of the response (minimizing the deviation $|\mathbf{y}-\tilde{\mathbf{y}}|$).

\subsection{Ordinary least squares \label{sect:OLS}}
In order to \textit{minimize} the error $\bm\varepsilon$, we must define exactly what that means. We require a functional expression\textemdash commonly referred to as the \textit{cost function}\textemdash and a metric in which to calculate it's size. Choosing the Euclidean $L^2$ norm ($\Vert \mathbf{v} \Vert_2=\sqrt{\sum_i v_i^2}$) and the absolute value of $\bm\varepsilon$ as the metric and cost function, respectively, leads to the \textit{ordinary least squares} (OLS) method. Defining the cost function, 
\begin{align}
C(\bm\beta) &= \Vert \mathbf{y} - \tilde{\mathbf{y}}\Vert_2^2 \nonumber \\
%
&= \Vert \mathbf{y} - \mathbf{X}\bm\beta\Vert_2^2 \nonumber \\ 
%%
&= \sum_{i=1}^n \Big| y_i - \beta_0 - \sum_{j=1}^p X_{ip} \beta_p \Big|^2, \label{eq:cost}
\end{align}
we can formulate the OLS method as computing $\bm\beta_\text{optimal}$ by
\begin{align}
\bm\beta_\text{optimal}=\underset{\bm\beta}{\text{arg\,min}}\big\{C(\bm\beta)\big\}.
\end{align}

In order to find $\bm\beta_\text{optimal}$, we may simply differentiate $C(\bm\beta)$ w.r.t.\ $\bm\beta$ and enforce $\partial C(\bm\beta)/\partial \bm\beta = 0$. Following Hastie, Tibshirani \& Friedman\autocite{trevor2009elements}, we find that
\begin{align}
\frac{\partial C(\bm\beta)}{\partial \bm\beta} &= \frac{\partial}{\partial \bm\beta} (\mathbf{y}-\mathbf{X}\bm\beta)^T (\mathbf{y}-\mathbf{X}\bm\beta) \nonumber \\
%
&= -2\mathbf{X}^T(\mathbf{y}-\mathbf{X}\bm\beta) \stackrel{!}{=} 0 \nonumber \\
%
\Rightarrow \mathbf{X}^T\mathbf{y} &= \mathbf{X}^T\mathbf{X}\bm\beta \nonumber \\
%
\bm\beta_\text{optimal} &= \left(\mathbf{X}^T\mathbf{X}\right)^{-1}\mathbf{X}^T\mathbf{y},
\end{align}
where we have written $\Vert \mathbf{y}-\mathbf{X}\bm\beta\Vert_2^2$ as $(\mathbf{y}-\mathbf{X}\bm\beta)^T (\mathbf{y}-\mathbf{X}\bm\beta)$. We note that even though $\mathbf{X}\in \mathbb{R}^{n\times p}$ is a "large" matrix (assuming the number of observations $n\gg p$ the number of predictors per observation), the product $\mathbf{X}^T\mathbf{X}\in \mathbb{R}^{p\times p}$ is "small". Thus explicitly inverting $\mathbf{X}^T\mathbf{X}$ is not a problem on a modern computer. 

We note that the \textit{model prediction} may now be calculated simply as $\tilde{\mathbf{y}}=\mathbf{X}\bm\beta_\text{optimal}$. This represents the optimal linear model subject under the Euclidean norm of the cost function as given in \eq{cost}. This method was first rigorously descriped by Legendre in 1805\autocite{legendre1805nouvelles}.

\subsubsection{The design matrix}
The design matrix, $\mathbf{X}$, can in principle contain any set of linearly independent functions of the predictors\footnote{We require linear independence to ensure the normal equations have a unique solution}. Every column in the design matrix corresponds to a mapping of the predictors, with elements $\mathbf{p}_i\mapsto \mathbf{X}_{ij}$. We will now consider an example of such a design matrix. We use two predictors\textemdash we will denote them $x$ and $y$\textemdash with the response $y$. We introduce our model using the mappings $(x,y)\mapsto x$, $(x,y)\mapsto y$, and $(x,y)\mapsto xy$. Including also what is commonly referred to as the intercept, this gives rise to the design matrix 
\begin{align}
\mathbf{X} &= \bmat{cccc}{
  1 & x_1 & y_1 & x_1y_1 \\
  1 & x_2 & y_2 & x_2y_2 \\
    &     & \vdots       \\
  1 & x_{n-1} & y_{n-1} & x_{n-1}y_{n-1} \\
  1 & x_n & y_n & x_ny_n}.
\end{align}
For inputs $(x_i,y_i)$ our model $\tilde{\mathbf{y}}$ now returns 
\begin{align}
\tilde{\mathbf{y}}_i &= \mathbf{x}^T_i\bm\beta \nonumber \\
%%
&= \beta_0 + \beta_1 x_i + \beta_2 y_i + \beta_3 x_iy_i,
\end{align}
where we used the shorthand notation $\mathbf{x}^T_i$ to denote $\text{Row}_i(\mathbf{X})$.


Before we continue describing the Ridge and Lasso regression schemes, we briefly introduce the basis sets used in this project.
\subsection{Polynomial basis sets}
Throughout the present work we employ a basis set of homogeneous monomials\footnote{A homogeneous polynomial is a polynomial in which all terms have the same total degree, e.g.\ $xy+y^2+x^2$ is a homogeneous polynomial, $x$ is a homogeneous monomial, while $xy+x^3$ is \textit{not}. Monomials are simply polynomials with only a single term.}. We will be working with 2D terrain data, and thus will need to consider monomials of up to and including two variables\textemdash $x$ and $y$\textemdash in all possible homogeneous combinations. Disregarding the zero degree monomial, there are two possible such terms of degree up to and including one. These are simply $x$ and $y$. Moving up to degree two, we must include $x^2$, $y^2$, and $xy$, for a total of five terms up to and including degree 2. Degree three adds an additional four terms: $x^3$, $x^2y$, $xy^2$, and $y^3$, and so on. In general, there are $n+1$ such terms for monomials of degree $n$, namely 
\begin{align}
x^n,\ x^{n-1}y, \ x^{n-2}y^2, \ \dots, \ xy^{n-1}, \ \text{and} \ y^n. \nonumber
\end{align}

The total basis sets of all such monomials of degree \textit{up to and including} degree $n$\textemdash $\mathcal{B}_n$\textemdash thus contains 
\begin{align}
\text{size}(\mathcal{B}_n) \sum_{k=2}^{n+1}k = \frac{n(n+3)}{2}
\end{align}
terms. 
\subsection{Ridge regression \label{sect:ridge}}
As mentioned in section \ref{sect:OLS}, defining exactly what we mean by \textit{minimizing the error} requires a cost function and a metric. The previous choice of $C(\bm\beta)=\Vert \mathbf{y}-\tilde{\mathbf{y}}\Vert_2^2$ is obviously not the only possible one. In fact, a possibly superiour method may be devised by keeping the Euclidean $L^2$ norm, but including a term in the cost function which penalizes large values of $\beta_i$. Such an approach was first used in statistics by Hoerl \& Kennard \autocite{hoerl1970ridge}, but was proposed already in the 1940s by Andrey Tikhonov\autocite{tikhonov1943stability}. Taking the cost function to be 
\begin{align}
C_\text{T}(\bm\beta) = \Vert \mathbf{y}-\tilde{\mathbf{y}}\Vert_2^2 + \Vert \bm\Gamma \bm\beta \Vert_2^2
\end{align}
gives rise to a regressions scheme known as Tikhonov regularization. The Tikhonov matrix $\bm\Gamma$ governs the form of the regularization term. A simple case of $\bm\Gamma=\sqrt{\lambda} \mathds{1}$, favoring solutions with small (in the $L^2$ norm sense) values of the parameters $\bm\beta$, results in the \textit{Ridge regression} of Hoerl \& Kennard. The $\lambda\ge 0$ parameter here represents a tuneable penalty for large $\bm\beta$ values. We note that $\lambda=0$ recovers the OLS method of section \ref{sect:OLS}.

Writing out the cost function of Ridge regression, we find that 
\begin{align}
C_\text{R}(\bm\beta) &= \Vert \mathbf{y}-\tilde{\mathbf{y}}\Vert_2^2 + \lambda \Vert \bm\beta \Vert_2^2 \nonumber \\
%
&= \Vert \mathbf{y} - \mathbf{X}\bm\beta\Vert_2^2 + \lambda \Vert \bm\beta \Vert_2^2 \nonumber \\ 
%%
&= \sum_{i=1}^n \Big| y_i - \beta_0 - \sum_{j=1}^p X_{ip} \beta_p \Big|^2 + \lambda \sum_{j=1}^p \beta_j^2,
\end{align}
where we have left out the intercept $\beta_0$ from the regularization term. This is done to ensure the solutions do not explicitly depend on the zero point chosen for $\mathbf{y}$, i.e.\ adding a constant to each response value $y_i$ would not result in a simple shift of the predictions by the same amount\autocite{trevor2009elements}. 

In the same way as before, we may differentiate the cost function and enforce $C_\text{R}(\bm\beta)=0$ in order to find the optimal $\bm\beta$. We obtain
\begin{align}
\frac{\partial C_\text{R}(\bm\beta)}{\partial \bm\beta} &= \frac{\partial}{\partial \bm\beta} \Big[(\mathbf{y}-\mathbf{X}\bm\beta)^T (\mathbf{y}-\mathbf{X}\bm\beta) + \lambda \mathds{1} \bm\beta^T\bm\beta\Big] \nonumber \\
%
&= -2\mathbf{X}^T(\mathbf{y}-\mathbf{X}\bm\beta) +2\lambda\mathds{1}\bm\beta \stackrel{!}{=} 0 \nonumber \\
%
\Rightarrow \mathbf{X}^T\mathbf{y} &= \mathbf{X}^T\mathbf{X}\bm\beta +\lambda \mathds{1} \bm\beta \nonumber \\
%
\bm\beta^\text{R}_\text{optimal} &= \left(\mathbf{X}^T\mathbf{X}+\lambda\mathds{1}\right)^{-1}\mathbf{X}^T\mathbf{y}. \label{eq:ridgebeta}
\end{align}

\subsubsection{Singular-value decomposition \label{sect:svd}}
Before we continue, we introduce briefly the singular-value decomposition (SVD). Note carefully that \textit{any} $m\times n$ matrix $\mathbf{A}$ can be decomposed into a product like this. 
\begin{grayframe}
Let $\mathbf{A}$ be an $m\times n$ matrix with rank $r$. Then there exists an $m\times n$ matrix $\bm{\Sigma}$ in which the first $r$ diagonal entries are the singlar values of $\mathbf{A}$, $\sigma_1\ge\sigma_2\ge\dots\ge\sigma_r>0$, and there exist and $m\times m$ orthogonal matrix $\mathbf{U}$ and an $n\times n$ orthogonal matrix $\mathbf{V}$ such that 
\begin{align}
\mathbf{A}=\mathbf{U}\bm\Sigma \mathbf{V}^T.
\end{align}
All entries in $\bm \Sigma$ outside of the first $r$ diagonal elements are zero. The singular values of $\mathbf{A}$ denote the square roots of the eigenvalues of $\mathbf{A}^T\mathbf{A}$.\autocite{lay2012linear}
\end{grayframe}
The (first $r$ columns of the) matrix $\mathbf{U}$ contains the eigenvectors of $\mathbf{A}^T\mathbf{A}$ and represents an orthonormal basis for the column space of $\mathbf{A}$, $\text{Col}\,\mathbf{A}$. The (first $r$ columns of the) matrix $\mathbf{V}$ contains the eigenvectors of $\mathbf{A}\mathbf{A}^T$ and represents an orthonormal basis for the row space of $\mathbf{A}$, $\text{Row}\,\mathbf{A}$. The remaining $m-r$ and $n-r$ columns of $\mathbf{U}$ and $\mathbf{V}$ form orthonormal bases for $\text{Nul}\,\mathbf{A}$ and $\text{Nul}\,\mathbf{A}^T$.

Thus we can interpret the SVD loosely as finding the orthonormal bases of $\text{Col}\,\mathbf{A}$ and $\text{Row}\,\mathbf{A}$ such that application of $\mathbf{A}$ maps $\mathbf{v}_i\mapsto \sigma_i\mathbf{u}_i$.

\subsubsection{Ridge regression using the SVD}
If we consider the SVD of $\mathbf{X}$, $\mathbf{X}=\mathbf{U}\bm\Sigma \mathbf{V}^T$, and compute $\mathbf{X}^T\mathbf{X}$ we find that 
\begin{align}
\mathbf{X}^T\mathbf{X} &= \left(\mathbf{U}\bm\Sigma \mathbf{V}^T\right)^T \left(\mathbf{U}\bm\Sigma \mathbf{V}^T\right) \nonumber \\
%
&= \mathbf{V}\bm\Sigma^T \mathbf{U}^T \mathbf{U}\bm\Sigma \mathbf{V}^T \nonumber \\
%
&= \mathbf{V}\bm\Sigma^T \bm\Sigma \mathbf{V}^T = \mathbf{V}\bm\Sigma^2\mathbf{V}^T,
\end{align}
where the orthogonality of $\mathbf{U}$ made $\mathbf{U}^T\mathbf{U}=\mathds{1}$. Inserting this into the expression for the optimal $\bm\beta_\text{optimal}^\text{R}$ (\eq{ridgebeta}) yields\autocite{trevor2009elements}
\begin{align}
\bm\beta^\text{R}_\text{optimal} &= \left(\mathbf{X}^T\mathbf{X}+\lambda\mathds{1}\right)^{-1}\mathbf{X}^T\mathbf{y} \nonumber \\
%
&= \left(\mathbf{V}\bm\Sigma^2\mathbf{V}^T +\lambda\mathds{1}\right)^{-1}\mathbf{X}^T\mathbf{y} \nonumber \\
%
&= \left(\mathbf{V}\bm\Sigma^2\mathbf{V}^T +\lambda\mathds{1}\mathbf{V}\mathbf{V}^T\right)^{-1}\mathbf{X}^T\mathbf{y} \nonumber \\
%
&= \left(\mathbf{V}\bm\Sigma^2\mathbf{V}^T +\lambda\mathbf{V}\mathds{1}\mathbf{V}^T\right)^{-1}\mathbf{X}^T\mathbf{y}, \nonumber
\end{align}
where we multiplied by $\mathds{1}=\mathbf{V}\mathbf{V}^T$ (recall that $\mathbf{V}^{-1}=\mathbf{V}^T$ due to orthogonality) and used the fact that the identity matrix commutes with any other matrix. Furthermore, we find
\begin{align}
\bm\beta^\text{R}_\text{optimal}&= \left(\mathbf{V}\left[\bm\Sigma^2 +\lambda\mathds{1}\right]\mathbf{V}^T\right)^{-1}\mathbf{X}^T\mathbf{y} \nonumber \\
%
&= \mathbf{V}\left[\bm\Sigma^2 +\lambda\mathds{1}\right]^{-1}\mathbf{V}^T\mathbf{X}^T\mathbf{y}  \label{eq:inv}\\
%
&= \mathbf{V}\left[\bm\Sigma^2 +\lambda\mathds{1}\right]^{-1}\mathbf{V}^T\left(\mathbf{U}\bm\Sigma \mathbf{V}^T\right)^T\mathbf{y} \nonumber \\
%
&= \mathbf{V}\left[\bm\Sigma^2 +\lambda\mathds{1}\right]^{-1}\mathbf{V}^T \mathbf{V}\bm\Sigma^T \mathbf{U}^T \mathbf{y} \nonumber \\
%
&= \mathbf{V}\left[\bm\Sigma^2 +\lambda\mathds{1}\right]^{-1}\bm\Sigma \mathbf{U}^T \mathbf{y},
\end{align}
where we used that $\bm\Sigma$ is diagonal, so $\bm\Sigma^T =\bm\Sigma$. Note that since $(\mathbf{A}\mathbf{B})^{-1}=\mathbf{B}^{-1}\mathbf{A}^{-1}$, we can rewrite the inverse product 
\begin{align}
\left(\mathbf{V}\left[\bm\Sigma^2+\lambda\mathds{1}\right]\mathbf{V}^T\right)^{-1} &= \left(\mathbf{V}^T\right)^{-1} \left[\bm\Sigma^2+\lambda \mathds{1}\right]^{-1}\mathbf{V}^{-1} \nonumber \\
%
&= \mathbf{V}\left[\bm\Sigma^2+\lambda \mathds{1}\right]^{-1}\mathbf{V}^T, \nonumber
\end{align} 
as was done in \eq{inv}. Since we are now taking the inverse of a diagonal matrix, we can simply write out the terms. Note that the inverse will itself be diagonal, and given by
\begin{align}
\left(\left[\bm\Sigma^2+\lambda\mathds{1}\right]^{-1}\right)_{ii} = \frac{1}{\sigma_{ii}^2+\lambda}.
\end{align}
Rewriting the OLS scheme in terms of the SVD yields a very similar 
\begin{align}
\bm\beta_\text{optimal} &= \mathbf{V}\left(\bm\Sigma^2\right)^{-1}\mathbf{V}^T \bm\Sigma^T \mathbf{U}^Ty.
\end{align}

\subsection{Lasso regression \label{sect:lasso}}
As mentioned repeatedly, we are free to choose what we mean by \textit{minimizing the error} w.r.t.\ what metric and what cost function we use. Whereas the Ridge regression of section \ref{sect:ridge} used $L^2$ regularization by adding a $\lambda\Vert \bm\beta\Vert_2^2$ term to $C(\bm\beta)$, we may instead try a $L^1$ regularization. This constitutes setting up the cost function as
\begin{align}
C_\text{L}(\bm\beta) &= \Vert \mathbf{y} - \tilde{\mathbf{y}}\Vert_2^2 + \lambda \Vert \bm\beta \Vert_1 \nonumber \\
%
&= \Vert \mathbf{y} - \mathbf{X}\bm\beta\Vert_2^2 + \lambda \Vert \bm\beta \Vert_1 \nonumber \\
%
&= \sum_{i=1}^n\Big| y_i - \beta_0 - \sum_{j=1}^p X_{ip}\beta_p\Big|^2 + \lambda \sum_{j=1}^p |\beta_j|^2.
\end{align}
Originally popularized by Tibshirani\autocite{tibshirani1996regression}, the \textit{Lasso regression} has certain potential advantages over the OLS and Ridge regression schemes. Most notably, the Lasso can perform \textit{variable selection}, i.e.\ some $\beta_j$s may be identically zero as a result of the minimization. The name Lasso is short for "least absolute shrinkage and selection operator".

Computing the derivative of the Lasso cost function yields
\begin{align}
\frac{\partial C_\text{L}(\bm\beta)}{\partial \bm\beta_j} &=  \frac{\partial C_\text{OLS}(\bm\beta)}{\partial \bm\beta_j} + \lambda \sum_{j=1}^p \frac{\partial}{\partial \beta_j} |\beta_j| \nonumber \\
%
&= \frac{\partial C_\text{OLS}(\bm\beta)}{\partial \bm\beta_j} + \lambda\frac{\beta_j}{\sqrt{\beta_j^2}} \nonumber \\
%
&= \frac{\partial C_\text{OLS}(\bm\beta)}{\partial \bm\beta_j} + \lambda\,\text{sgn}(\beta_j) \stackrel{!}{=} 0.
\end{align}
In general, this can not be directly solved for $\bm\beta_\text{optimal}^\text{L}$ as in the case of OLS or the Ridge scheme. Under the assumption that $\mathbf{X}$ is orthogonal, an explicit solution exists and is given by\autocite{mehta2018highbias}
\begin{align}
\left(\beta_\text{optimal}^\text{L}(\lambda)\right)_j &= \text{sgn}(\beta_j^\text{OLS})\left(|\beta_j^\text{OLS}| - \lambda\right)_+,
\end{align}
where $(\cdot)_+$ represents the positive part of $\cdot\,$. In the general case, an iterative solver must be used to compute $\bm\beta_\text{optimal}^\text{L}$.

\subsection{Principal components \label{sect:principal}}
The following section follows Hastie, Tibshirani \& Friedman\autocite{trevor2009elements}.

Recall from section \ref{sect:svd} that the SVD matrix $\mathbf{U}$ represents an orthonormal basis for the column space of the decomposed matrix $\mathbf{A}$. If we consider the prediction resulting from OLS, and perform a SVD of $\mathbf{X}$ we find that
\begin{align}
\tilde{\mathbf{y}}_\text{OLS} &= \mathbf{X}\bm\beta_\text{OLS} \nonumber \\
%
&= \mathbf{X}\left(\mathbf{X}^T\mathbf{X}\right)^{-1}\mathbf{X}^T\mathbf{y} \nonumber \\
%
&= \mathbf{X} \left[ \left(\mathbf{U}\bm\Sigma \mathbf{V}^T\right)^T \mathbf{U}\bm\Sigma \mathbf{V}^T \right]^{-1} \mathbf{X}^T \mathbf{y} \nonumber \\
%
&= \mathbf{U}\bm\Sigma \mathbf{V}^T \left[\mathbf{V}\bm\Sigma^2 \mathbf{V}^T \right]^{-1} \mathbf{V}\bm\Sigma^T \mathbf{U}^T\mathbf{y} \nonumber \\
%
&= \mathbf{U}\bm\Sigma \mathbf{V}^T \mathbf{V} \left[\bm\Sigma^2  \right]^{-1}  \mathbf{V}^T \mathbf{V}\bm\Sigma^T \mathbf{U}^T\mathbf{y} \nonumber \\
%
&= \mathbf{U}\bm\Sigma \left[\bm\Sigma^2  \right]^{-1}  \mathbf{V}^T \mathbf{V}\bm\Sigma^T \mathbf{U}^T\mathbf{y} \nonumber \\
%
&= \mathbf{U}\bm\Sigma \left[\bm\Sigma^2  \right]^{-1} \bm\Sigma^T \mathbf{U}^T\mathbf{y} \nonumber \\
%
&= \mathbf{U}\mathbf{U}^T\mathbf{y}. \label{eq:principal1}
\end{align}
A similar derivation for $\tilde{\mathbf{y}}_\text{R}$ for the Ridge regression yields
\begin{align}
\tilde{\mathbf{y}}_\text{R} &= \mathbf{X}\bm\beta_\text{R} \nonumber \\
%
&= \mathbf{X}\left(\mathbf{X}^T\mathbf{X}+\lambda \mathds{1}\right)^{-1}\mathbf{X}^T\mathbf{y} \nonumber \\
%
&= \mathbf{U}\bm\Sigma\left(\bm\Sigma^2 + \lambda \mathds{1}\right)^{-1}\bm\Sigma \mathbf{U}^T\mathbf{y} \nonumber \\
%
&= \sum_{j=1}^p \mathbf{u}_j \frac{\sigma_j^2}{\sigma_j^2+\lambda}\mathbf{u}_j^T\mathbf{y}, \label{eq:principal2}
\end{align}
where $\sigma_j$ denotes the diagonal elements of the diagonal matrix $\bm\Sigma$, i.e.\ $(\bm\Sigma)_{jj}=\sigma_j$. Note now that $\mathbf{U}^T\mathbf{y}$ are the coordinates of $\mathbf{y}$ w.r.t.\ the orthonormal basis $\mathbf{U}$. Comparing Eqs. (\ref{eq:principal1}) and (\ref{eq:principal2}), we note that the coordinates of $\mathbf{y}$ in both cases are computed in the orthonormal basis of $\text{Col}\,\mathbf{X}$ (as specified by the SVD matrix $\mathbf{U}$), but the Ridge scheme also \textit{shrinks} the coordinates. The shrinkage is large whenever $\sigma_j^2$ is small. Recalling that $\mathbf{X}^T\mathbf{X}=\mathbf{V}\bm\Sigma^2\mathbf{V}^T$ is an eigendecomposition of $\mathbf{X}^T\mathbf{X}$, with $\mathbf{V}$ containing the eigenvectors, we denote these eigenvectors $\mathbf{v}_j$ to be the \textit{principal components} of $\mathbf{X}$. The eigenvalues contained in $\bm\Sigma^2$ are precicely the proportionality factors, $\sigma^2_j$, involved in the shrinkage. 

The diagonalization of $\mathbf{X}^T\mathbf{X}$ constitutes a coordinate transform into a coordinate system in which $\mathbf{X}^T\mathbf{X}$ itself is diagonal. The matrix $\mathbf{X}^T\mathbf{X}$ is the covariance matrix (apart from a constant factor $1/N$). In the orthonormal basis of $\mathbf{V}$, the covariance matrix is diagonal and contains the variances $\sigma_j^2$ of these linear combinations of the columns of $\mathbf{X}$.

The first principal component direction has the property that $\mathbf{z}_1=\mathbf{X}\mathbf{v}_1$ has the largest variance of all the linear combinations of the columns of $\mathbf{X}$. In general, the principal components are ordered such that $\text{Var}\,\mathbf{z}_1\ge \text{Var}\,\mathbf{z}_2\ge\dots \ge\text{Var}\,\mathbf{z}_N$.

In essence, the first principal component represent the direction in $\text{Col}\,\mathbf{X}$ in which the variance is highest, the second principal component represents the corresponding direction in which the variance is highest, apart from the first, and so on. It is clear that the Ridge regression scheme simply rotates the OLS solution into the principal components of the design matrix, and then shrinks the coefficients corresponding to low-variance components. 

\subsection{Assessing model accuracy \label{sect:accuracy}}
The goal in machine learning is to use a chosen model to make predictions. In particular, we need to quantify how close 
a predicted response value for a given observation is to the true response value.
In the regression setting, a common measure is the \textit{mean squared error} (MSE), given by 
\begin{equation}
 \text{MSE} = \frac{1}{n} \sum_{i=1}^n \left( y_i - \hat{f}(x_i) \right)^2,
\end{equation}
where $\hat{f}(x_i)$ is the prediction for the ith observation $y_i$. Computing the MSE on the training set result in what we 
could call the training MSE and gives a measure of how well the model fits the training data. However, we are mainly interested 
in how good predictions are on previously unseen data.

Suppose that we have set of test data, $\{x_i,y_i\}$, which is not used during training. Then we could compute the MSE on 
the test set, giving a measure on how well the method works on unseen data. We would like to select the method which has 
the lowest test MSE. 

The challenge now is how do we estimate the test MSE? Of course, if test data are available we just compute. The problem is  
that test data are not available in general. So if do not have test data at hand, a natural approach seems to choose 
the method that minimizes the training MSE. However, there is no guarantee that low training MSE results in the lowest test MSE. 
As an extreme example suppose you draw a curve that passes through every point in a data set, this would result in perfect train 
MSE. If you try to use this model on another data set where I move one datapoint significantly it would result in a high test MSE.

One can show that the expected test MSE, for a given value $x_0$, can always be decomposed as 
\begin{align}
 E[(y_0 - \hat{f}(x_0))]^2 &= \text{Var}(\hat{f}(x_0)) + [\text{Bias}(\hat{f}(x_0))]^2 \nonumber \\
 %
 & \ \ \ \ \ \ \  \ \ \ \ \ \ \ + \text{Var}(\epsilon) \label{eq:BiasVarTradeOff}.
\end{align}
Here $E[(y_0 - \hat{f}(x_0))]^2$ defines the expected test MSE, and refers
to the average test MSE that we would obtain if we repeatedly estimated $f$ using a large number of training sets, 
and tested each at $x_0$. 

The variance, $\text{Var}(\hat{f}(x_0))$, measures how much $\hat{f}$ would change if we trained the 
model on a different data set. Ideally $\hat{f}$ should not vary to much between training sets. Generally more flexible methods 
have higher variance, which can give large deviations in $\hat{f}$ when changes in the training set is small.

Bias measures the the error introduced by assuming for example a linear relationship between the response $\mathbf{y}$ and the 
predictors $\mathbf{X}$. Thus, if the true relation is non-linear this will result in bias. In particular, if the data is highly 
non-linear it would be impossible to produce accurate estimates by assuming a linear relationship between the response and predictors.
Generally more flexible methods will have less bias. 

\eq{BiasVarTradeOff} suggests that we want both low variance and low bias in order to minimize the expected 
test error. The above discussion implies that these are conflicting interests, in the sense that increasing flexibility of a model 
reduces bias, but increases variance. This phenomena is referred to as the \textit{bias-variance trade-off}.

In practice, with test data unavailable, it is generally not possible to compute the test MSE, bias or variance for a specific 
method. Thus we need tools to estimate the test MSE (or other quantities of interest) from training data. Two such methods are 
cross-validation and the \textit{bootstrap}, which belongs to a class of methods called \textit{resampling} methods.

\subsection{Resampling methods \label{sect:resampling}}
Resampling methods is an important tool in statistics. The idea is to repeatedly draw samples from a training set and refit 
the model on each sample in order to gain additional information about the fitted model.

One drawback of resampling methods is that they can be computationally expensive, since they have to fit the same model 
multiple times using different subsets of the training data. In particular, if there is a large number of samples or if 
each instance of training is expensive this can be time consuming. However, the computational requirement of resampling methods 
are in general not prohibitive. Two of the most commonly used resampling methods are cross-validation and 
the \textit{bootstrap}.

Cross-validation can be used to estimate the test error in order to assess the performance of a particular model, or to select 
the suitable level of flexibility. Evaluating a model's performance is known as \textit{model assessment}, while selecting the 
proper level of flexibility for a model is known as \textit{model selection}. The bootstrap is commonly used to provide 
a measure of accuracy of a parameter estimate.

\subsection{The \textit{bootstrap}}
The bootstrap method is a statistical technique for estimating quantities about a population by taking averages 
over estimates from several small data samples. Samples are constructed by drawing observations from a large 
data sample -in our case the training set- with replacement. The bootstrap method estimates a quantity of a 
population by taking small samples repeatedly, calculating the statistic, 
and taking the average of the calculated statistics. In particular we use the bootstrap to estimate the MSE, bias and variance
which will give an indication of the appropriate order of complexity for a given model. 

\section{Data sets}
We are chiefly interested in parametrizing digital terrain data. However, in order to test and validate our implementation of the regression model and the resampling technique, we employ the Franke function\autocite{franke1979critical} as a test case before considering real data.

\subsection{The Franke function \label{sect:franke}}
The test function of Franke\textemdash originally developed to test and rate different surface interpolation techniques\textemdash is "a surface with a variety of behaviour" which consists of "two Gaussian peaks and a sharper Gaussian dip superimposed on a surface sloping towards the first quadrant."\autocite{franke1979critical} It is noted by Franke in the his original paper that the slope was introduced mainly as a visual aid and presumably had little impact on the actual interpolations performed. 

More specifically, the Franke function $f_\text{F}(x,y)$ takes the full form
\begin{align}
f_\text{F}(x,y) &= \frac{3}{4}\exp\left\{\frac{-1}{4}\left[\left(9x-2\right)^2 + \left(9y-2\right)^2\right]\right\}\nonumber \\
%%
&+ \frac{3}{4}\exp\left\{\frac{-1}{49}\left(9x+1\right)^2 + \frac{1}{10}\left(9y+1\right)^2\right\}\nonumber \\
%%
&+ \frac{1}{2}\exp\left\{\frac{-1}{4}\left[\left(9x-7\right)^2 + \left(9y-3\right)^2\right]\right\}\nonumber \\
%%
&- \frac{1}{5}\exp\left\{\frac{-1}{4}\left[\left(9x+4\right)^2 + \left(9y-7\right)^2\right]\right\}.
\end{align}
A plot of the $f_\text{F}(x,y)$ surface can be seen in \fig{1}.

\begin{figure}
\centering
\includegraphics[width=\linewidth]{franke.png}
\captionof{figure}{The Franke test function plotted for $0\le x,y\le 1$. \label{fig:1}}
\end{figure}

\subsection{Terrain data \label{sect:terraindata}}
The terrain data used is taken from the U.S. Department of the Interior U.S. Geological Survey's (USGS) EarthExplorer\footnote{EarthExplorer website: \url{https://earthexplorer.usgs.gov/}.} website. The USGS stores data from the Shuttle Radar Topography Mission (SRTM) which maps the earth's land surface topology with a resolution of 1 arc-second (about $30\,\text{m}$). We will use SRTM data taken from the EarthExplorer website as the basis for our terrain parametrization.

The specific terrain data we will use in the present project is taken from the Møsvatn Austfjell area in the municipality of Tinn in Telemark county, Norway. A visual representation of the data is shown in \fig{2}. 

\begin{figure}
\centering
\includegraphics[width=\linewidth]{terrain2.png}
\captionof{figure}{The terrain data in use in the present work, taken from the Møsvatn Austfjell area in the municipality of Tinn in Telemark county, Norway. Retrieved using the USGS EarthExplorer website. The height data is scaled to fit in $0\le z\le 1$, and the reference zero point is set to zero. \label{fig:2}}
\end{figure}


\section{Results and discussion}
\subsection{Verification of the models: The Franke function}
\newlength{\myrowskip}
\setlength{\myrowskip}{10pt}
\begin{table*}
\centering\sisetup{table-number-alignment=center}
\setlength\extrarowheight{2pt}
\begin{tabularx}{\textwidth}{X*{9}{S[table-format=4.3]}}
\toprule
$p$ & \multicolumn{2}{>{\hsize=\dimexpr2\hsize}c}{\ \ \ \ $2$} 
    & \multicolumn{2}{>{\hsize=\dimexpr2\hsize}c}{\ \ \ \ $3$}                        
    & \multicolumn{2}{>{\hsize=\dimexpr2\hsize}c}{\ \ \ \ $4$}                            
    & \multicolumn{2}{>{\hsize=\dimexpr2\hsize}c}{\ \ \ \ $5$} \\
%%
    & \multicolumn{1}{>{\hsize=\dimexpr1\hsize}r}{$\beta$} 
    & \multicolumn{1}{>{\hsize=\dimexpr1\hsize}r}{$\sigma^2(\beta)$}   
    & \multicolumn{1}{>{\hsize=\dimexpr1\hsize}r}{$\beta$} 
    & \multicolumn{1}{>{\hsize=\dimexpr1\hsize}r}{$\sigma^2(\beta)$} 
    & \multicolumn{1}{>{\hsize=\dimexpr1\hsize}r}{$\beta$} 
    & \multicolumn{1}{>{\hsize=\dimexpr1\hsize}r}{$\sigma^2(\beta)$}
    & \multicolumn{1}{>{\hsize=\dimexpr1\hsize}r}{$\beta$}  
    & \multicolumn{1}{>{\hsize=\dimexpr1\hsize}r}{$\sigma^2(\beta)$} \\
%%
\midrule
$1$       &  1.17606 & 0.00007 &  0.99432 & 0.00014 &   0.60714 & 0.00006 &   0.37920 & 0.00052 \\[\myrowskip]
$x$       & -1.06533 & 0.00047 & -0.60390 & 0.00314 &   4.23195 & 0.00478 &   8.05787 & 0.03989 \\
$y$       & -0.74196 & 0.00041 &  1.36710 & 0.00206 &   3.18895 & 0.00399 &   3.77385 & 0.03382 \\[\myrowskip]
$x^2$     &  0.12032 & 0.00026 & -1.40195 & 0.00885 & -19.38216 & 0.05541 & -35.01195 & 0.52045 \\
$xy$      &  0.87501 & 0.00025 &  2.04306 & 0.00597 &  -2.28349 & 0.02825 & -15.64235 & 0.50091 \\
$y^2$     & -0.36720 & 0.00029 & -6.65928 & 0.00585 & -12.53672 & 0.04434 &  -8.30869 & 0.44047 \\[\myrowskip]
$x^3$     &          &         &  0.89177 & 0.00288 &  25.25628 & 0.10794 &  49.21854 & 1.61746 \\
$x^2y$    &          &         &  0.36261 & 0.00219 &   8.10444 & 0.05690 &  45.87508 & 1.70397 \\
$xy^2$    &          &         & -1.50662 & 0.00188 &   1.40554 & 0.05242 &  21.25127 & 1.47419 \\
$y^3$     &          &         &  4.69644 & 0.00207 &  12.61520 & 0.08471 &  -9.03064 & 1.50607 \\[\myrowskip]
$x^4$     &          &         &          &         & -10.84136 & 0.02548 & -24.13509 & 1.25784 \\
$x^3y$    &          &         &          &         &  -5.15114 & 0.01948 & -54.81283 & 1.39124 \\
$x^2y^2$  &          &         &          &         &   0.00764 & 0.01437 &  -8.23581 & 1.14134 \\
$xy^3$    &          &         &          &         &  -1.90524 & 0.02013 & -30.19186 & 1.12319 \\
$y^4$     &          &         &          &         &  -3.51179 & 0.02032 &  30.20166 & 1.28260 \\[\myrowskip]
$x^5$     &          &         &          &         &           &         &   1.53882 & 0.16041 \\
$x^4y$    &          &         &          &         &           &         &  19.53504 & 0.18567 \\
$x^3y^2$  &          &         &          &         &           &         &  10.83952 & 0.17047 \\
$x^2y^3$  &          &         &          &         &           &         &  -5.29938 & 0.15935 \\
$xy^4$    &          &         &          &         &           &         &  16.91243 & 0.15035 \\
$y^5$     &          &         &          &         &           &         & -16.84369 & 0.16920 \\
\bottomrule 
\end{tabularx}
\caption{Parameters $\beta$ and their bootstrap computed variance $\sigma^2(\beta)$ for the OLS fits of the Franke function, shown in \fig{3}. Pair wise columns represent $\beta$ and $\sigma^2(\beta)$ for each polynomial degree $p$ used. Each row shows the $\beta_j$ coefficient and $\sigma^2(\beta)$ for the corresponding monomial${}_j$ term. \label{tab:1}}
\end{table*}
\begin{figure*}[p]
\vspace{-50pt}
\centering
\subfloat[$p=2$]{\includegraphics[width=0.4\linewidth]{OLS2.png}}
\subfloat[$p=3$]{\includegraphics[width=0.4\linewidth]{OLS3.png}} \\[-20pt]
\subfloat[$p=4$]{\includegraphics[width=0.4\linewidth]{OLS4.png}}
\subfloat[$p=5$]{\includegraphics[width=0.4\linewidth]{OLS5.png}}
\captionof{figure}{Ordinary least squares fits, using data from the Franke function with polynomials of degree $2$, $3$, $4$, and $5$. The $p$ parameter indicates what order of polynomials are used. The target function of Franke can be seen in \fig{1}. \label{fig:3}}
\end{figure*}
\begin{figure*}[p]
\centering
\subfloat[$p=2$]{\includegraphics[width=0.4\linewidth]{OLS2_diff.png}}
\subfloat[$p=3$]{\includegraphics[width=0.4\linewidth]{OLS3_diff.png}} \\[-20pt]
\subfloat[$p=4$]{\includegraphics[width=0.4\linewidth]{OLS4_diff.png}}
\subfloat[$p=5$]{\includegraphics[width=0.4\linewidth]{OLS5_diff.png}}
\captionof{figure}{The absolute difference between the ordinary least squares fits of \fig{3} and the true data, the Franke function. Polynomials of degree $2$, $3$, $4$, and $5$ have been used in the fitting. The $p$ parameter indicates what order of polynomials are used. \label{fig:4}}
\end{figure*}
Our first tests comprise simple OLS fitting of the Franke function described in section \ref{sect:franke}. We perform the fits using the scheme described in section \ref{sect:OLS}. Basic testing is done automatically using the \texttt{pytest} unit test framework.\footnote{All tests can be run automatically by calling \texttt{pytest -v} from anywhere within the Github respository} These ensure e.g.\ that the fits performed by the manual matrix inversion are identical to the ones performed automatically by sci-kit learn, that intercepts and polynomial coefficients are fitted correctly when the underlying functional form is itself a polynomial of known order, etc. In addition to the small scale automatic testing, we perform more thorough, extensive testing on the results of the fittings. First off we visualize and compare the performed OLS fits to the underlying Franke function. These are shown in \fig{3} (the Franke function, for reference, is shown in \fig{1}). We note that, as expected, higher degree polynomials appear to recreate the underlying exponentials better than lower degree polynomials. 

The fact that higher order polynomials are better able to reproduce the Franke function is in itself not surprising. Remember that Franke formed his function from a sum of exponentials. The power series expansion of this function is thus just a sum over all possible monomials in $x$ and $y$ with prefactors proportional to $1/n!$. 

The resampling scheme described in section \ref{sect:resampling}, we compute the variance of the computed $\beta$coefficients. These are shown in \tab{1} for all the different polynomial degrees used, ranging from $p=2$ to $p=5$. We note that increasing the polynomial degrees apparently tends to increase the variance of the computed $\beta$ coefficients. This is in line with the statement of the bias-variance tradeoff: Higher model complexity leads to an increased sensitivity to small fluctuations in the training set, and the variance goes up. This is illustrated in \fig{5}, where the mean values of the variances of all $\beta$ parameters corresponding to a total monomial degree is plotted for each model. 

\begin{figure}
\centering
\includegraphics[width=\linewidth]{beta_variance_OLS.png}
\captionof{figure}{Mean values of the variances of the $\beta$ coefficients resulting from OLS fitting, e.g.\ $\text{mean} \,\sigma^2(\beta_2)$ is the mean of all $\beta$s corresponding to monomials of total degree two, i.e. $\beta_{x^2}$, $\beta_{xy}$, and $\beta_{y^2}$. We note that as the overall complexity of the model increases (increasing $p$), the variance of each family of $\beta$s tends to increase. \label{fig:5}}
\end{figure}

The absolute difference between the fitted models and the Franke function is shown in \fig{4}. We note that the magnitude of the error decreases as the polynomial order increases, but the error for higher order polynomial fits exhibit faster oscillating behaviour. 

The mean squared error, introduced in section \ref{sect:accuracy} gives a measure of the accuracy of each model in a single number, the expectation value of the squared difference between each model prediction and the true underlying function value at each data point. We compute the MSE and the \textit{coefficient of determination}, the $R^2$ score, as a function of the polynomial degrees used. This is shown in \fig{6} (note that $1-R^2$ is shown, not $R^2$ itself). As expected, both decrease as model complexity increases. We note that the MSE and the $R^2$ score are approximately proportional to $\propto 10^{-p/4}$ in the range plotted here.

\begin{figure*}
\centering
\includegraphics[width=0.49\linewidth]{OLS_R2.png}
\includegraphics[width=0.49\linewidth]{OLS_MSE.png}
\captionof{figure}{Mean squared error and coefficient of determination, $R^2$, for the ordinary least squares fits on the Franke function. The parameter $p$ indicates the order of polynomials used in the fitting. \label{fig:6}}
\end{figure*}

\subsubsection{Noisy Franke function}
The next step in our validation scheme is to investigate how our OLS handles normally distributed noise added to the underlying functional values. Instead of $f_\text{F}(x,y)$ as we have fitted against previously, we now consider
\begin{align}
\tilde{f}_\text{F}(x,y) &= f_\text{F}(x,y) + \eta \mathcal{N}(0,1),
\end{align}
where $\mathcal{N}(\mu,\sigma)$ is normally distributed stochastic noise with mean $\mu=0$ and variance and standard deviation $\sigma=\sigma^2=1$. The parameter $\eta$ determines the size of the noise, i.e. the signal to noise ratio (since the Franke function takes values on the order of $\propto 1$). As an illustrative example, we consider the case of $p=2$, with only five $\beta$ parameters and perform an OLS fit on the noisy $\tilde{f}_\text{F}(x,y)$ function. The resulting $\sigma^2(\beta_j)$ values are shown in \fig{7}, where we have used the Bootstrap scheme to extract the variances. 

\begin{figure}
\centering
\includegraphics[width=\linewidth]{beta_variance_OLS_noise.png}
\captionof{figure}{The variance of the $\beta_j$ coefficients of a $p=2$ OLS fit on the noisy Franke function $\tilde{f}_\text{F}(x,y)$ with signal to noise ratio $\eta$. A total of $k=10\,000$ Bootstrap samples were used to calculate the variances for each value of $\eta$. \label{fig:7}}
\end{figure}

The variance increases as the noise overlying the Franke function increases. This is not unexpected, but it may be more interested to investigate how well the OLS model is able to fit the underlying function when the noise is included in the fitting. In order to visualize this, we plot $R^2$ and the mean squared error as functions of the noise scale factor $\eta$, \textit{and} the corresponding $R^2$ and MSE values \textit{relative to the plain Franke function with no noise}. For convenience we denote the $R^2$ score and the MSE values relative to the Franke function with no noise $R^2_{\eta=0}$ and MSE$_{\eta=0}$.

If the model is able to fit the underlying function despite the noise, then the values of $1-R^2_{\eta=0}$ and MSE${}_{\eta=0}$ will remain low even as $1-R^2$ and the MSE relative to the noisy Franke function increases. Once $1-R^2_{\eta=0}$ and MSE${}_{\eta=0}$ start increasing, it means the noise is too much for the OLS scheme to handle and the fit is no longer representative of the underlying function. An example of this is shown in \fig{8}, where a $p=10$ order polynomial was used to fit for different $\eta$ values. We note that the dotted lines ($1-R^2_{\eta=0}$ and MSE${}_{\eta=0}$) remain essentially unchanged up to $\eta\approx 0.1$, meaning a signal to noise ratio of $\propto10$ is easily handled by our model. As $\eta$ increases further, the fit to the underlying Franke function becomes poorer, but surprisingly at $\eta=1$\textemdash at which the size of the noise is on the order of the size of the function itself\textemdash the model still represents a good fit to $f_\text{F}(x,y)$ at MSE$\approx10^{-3}$. Comparing to \fig{6} we see that this is close to the MSE of $p=6$ degree fit with $\eta=0$.

\begin{figure}
\centering
\includegraphics[width=\linewidth]{R2MSE_OLS_noise.png}
\captionof{figure}{The mean squared error and $1$ minus the coefficient of determination $R^2$ as a function of the noise scale factor $\eta$. The dotted line represents the MSE and the $R^2$ score \textit{relative to the underlying Franke function with no noise}. The full line with circles shows the corresponding values for the Franke function with the noise (the function the fit was performed on). A $p=10$ order polynomial was used in this OLS fit.\label{fig:8}}
\end{figure}



\subsubsection{Ridge regression on the Franke function}
Having investigated the behaviour of the OLS scheme in some detail, we now turn to the Ridge regression method. Since this introduces a shrinkage parameter $\lambda$ it is natural to start with exploring how the MSE and the coefficient of determination depends on it. The MSE's dependence on the shrinkage parameter is shown in \fig{9}. We note\textemdash somewhat surprisingly\textemdash that for no value of $\lambda$ is the computed MSE value lower than that of the ordinary least squares method ($\lambda=0$). This is true over the entire spectrum of noise scales considered, $\eta=10^{-3}$ to $\eta=1$. We note that as the noise-to-signal ratio increases, the dependence on the shrinkage parameter $\lambda$ lessens, and at $\eta=1$ the model shows essentially no dependence on $\lambda$ (i.e.\ the OLS and Ridge regressions for all $\lambda$ values result in the same prediction).

The corresponding plot of the coefficient of determination, $R^2$, shows behaviour equal to that of \fig{9} and is therefore omitted from the present work.

\begin{figure}
\centering
\includegraphics[width=\linewidth]{MSE_ridge_noise.png}
\captionof{figure}{The mean squared error as a function of the signal-to-noise ratio $\eta$ for Ridge regression at various different shrinkage values $\lambda$. The OLS result (corresponding to $\lambda=0$) is inset for comparison. A $p=5$ order polynomial was used in these fittings. \label{fig:9}}
\end{figure}

As discussed in section \ref{sect:principal}, the Ridge regression method shrinks the $\beta_j$ parameters corresponding to low-variance principal components of the OLS solutions. We can visualize the shrinkage done in the Ridge regression scheme by considering the $\beta_j$ parameters as a function of $\lambda$ for some fixed model. In order to keep the number of $\beta_j$s relatively low, we consider in the follwing a $p=3$ polynomial model. We set the noise factor to $\eta=1.0$ and consider the sizes of the parameters $\beta_j$ for varying $\lambda$. This is shown in \fig{10}. Note carefully that the Ridge scheme is unable to set individual $\beta_j$s to zero, but rather shrinks all $\beta$ values. This is as expected, c.f.\ section \ref{sect:lasso}.

\begin{figure}
\centering
\includegraphics[width=\linewidth]{beta_ridge.png}
\captionof{figure}{The coefficients $\beta_j$ for varying shrinkage parameter $\lambda$ for Ridge regression fitting of the noisy Franke function with noise-to-signal ratio $\eta=1.0$. The error bars represent the confidence intervals for each $\beta_j$, calculated as $\beta_j\pm2\sigma^2(\beta_j)$, where $\sigma^2(\beta_j)$ is the Bootstrap computed variance for each $\beta_j$. Note carefully: The disconnected first point plotted at $\lambda=10^{-3}$ represents the ordinary least squares $\beta_j$s and their confidence intervals. The number of Bootstrap samples used to compute the variance in every case was $k=10\,000$. \label{fig:10}}
\end{figure}

\subsubsection{Lasso regression on the Franke function}
Finally, we consider the Lasso regression scheme applied to the Franke function. As with the Ridge method, we first take a look at the MSE as a function of the shrinkage factor $\lambda$. This is shown in \fig{12}. As with the Ridge regression, we note that no value of $\lambda$ gives better MSE results than OLS.
\begin{figure}
\centering
\includegraphics[width=\linewidth]{MSE_lasso_noise.png}
\captionof{figure}{The mean squared error as a function of the signal-to-noise ratio $\eta$ for Lasso regression at various different shrinkage values $\lambda$. The OLS result (corresponding to $\lambda=0$) is inset for comparison. A $p=8$ order polynomial was used in these fittings. \label{fig:12}}
\end{figure}

Whereas Ridge regression is unable to selectively put $\beta_j$ parameters to zero, the Lasso regression can perform variable selection as well as regularization. \fig{13} illustrates this. It is the corresponding Lasso regression version of \fig{10}, showing the evolution of $\beta_j$ as $\lambda$ increases. The same degree $p=3$ basis as was used in the Ridge case is also used in \fig{13}.
\begin{figure}
\centering
\includegraphics[width=\linewidth]{beta_lasso.png}
\captionof{figure}{The coefficients $\beta_j$ for varying shrinkage parameter $\lambda$ for Lasso regression fitting of the noisy Franke function with noise-to-signal ratio $\eta=1.0$. The error bars represent the confidence intervals for each $\beta_j$, calculated as $\beta_j\pm2\sigma^2(\beta_j)$, where $\sigma^2(\beta_j)$ is the Bootstrap computed variance for each $\beta_j$. Note carefully: The disconnected first point plotted at $\lambda=10^{-4}$ represents the ordinary least squares $\beta_j$s and their confidence intervals. The number of Bootstrap samples used to compute the variance in every case was $k=100$. \label{fig:13}}
\end{figure}


\subsubsection{Bias-variance tradeoff}
\begin{figure}
\centering
\includegraphics[width=\linewidth]{bias_variance_tradeoff_2.png}
\captionof{figure}{The mean squared error, the bias, and the variance for Ridge regression with $\lambda=1$ on the Franke function with noise scale $\eta=0.3$. The number of Bootstrap samples used to calculate the variance is $k=100$. Note carefully the different axes for the variance (right hand side of the figure). \label{fig:11}}
\end{figure}
As discussed in section \ref{sect:accuracy}, increasing the flexibility of the model increases the variance but reduces the bias. Since the expected value of the error\textemdash the MSE\textemdash is the sum of the bias, the varaince, and the irreducible error, there must exist some sweet spot in which the total MSE is minimal. It is customary to plot the MSE, the bias, and the variance in a single plot and observe that as the bias initially decreases as complexity increases, so does the MSE. As complexity increases too much, and the variance begins to dominate the total MSE (overfitting), the error will start to increase. The bias-variance sweetspot is then found somewhere in the middle: The model complexity which minimizes the MSE. For the noisy Franke function we present such a plot in \fig{11}. 

Note that, although the overall shape of the three lines are correct, the variance rises extremely slowly in comparison to the bias (note carefully the different vertical scales). Because of this, it appears that higher complexity is objectively better for the current problem example with the Franke function, up to and including polynomial degrees $p=20$. Since we are computationally restricted to relatively low polynomial degrees in the present work, we are not able to explore regions $p>20$ for which the variance eventually begins to dominate the MSE expression. 

This may be the reason why we find that the OLS scheme is strictly better (in terms of the MSE) than both the Ridge and Lasso methods in the present work: We are simply in a complexity region in which the variance is a non-issue for the MSE as a whole. Essentially, we are too far from overfitting our model, that $L^1$ and $L^2$ regularization (and even the variable selection of Lasso regression) does not benefit us much.

\subsection{Terrain data parametrization \label{sect:terrain_param}}
We turn now to terrain data parametrization, and use a subsection of the data shown in section \ref{sect:terraindata} and \fig{2}. The raw data set is $3601\times 1801$ points, of which we extract every $60$ points in the first dimension and every $30$ in the second. This results in a training set of size $60\times 60$. For testing purposes, we extract a similar $60\times60$ testing data set by performing the same subsectioning only with indices shifted $30$ points in each direction. This yields $3600$ training points and $3600$ testing points on which to evaluate our models. 

In order to give our models the best possible chance of giving good fits, we prepare the data set by normalizing it and subtracting the minimum value, i.e.\ for every $(x,y)$ point and height $z$ we perform the following assignments:
\begin{align}
z \ &\leftarrow \ z-\min_\text{all points}(z) \\
z \ &\leftarrow \ \frac{z}{\max_\text{all points}(z)} .
\end{align}
We consider the $x$ and $y$ coordinates to take values in $(0,1]$ as for the Franke function of section \ref{sect:franke}.

Having already developed and extensively tested our methods, all that remains is simply to apply OLS, Ridge, and Lasso regression to this data set. The resulting fits for $\lambda_\text{R}=10^{-3}$ and $\lambda_\text{L}=10^{-4}$ is shown in \fig{14}. These shrinkage factors are simply used as an example. A comparison of the MSE calculated on the same data set using different values of $\lambda$ is shown in \fig{15}. We note that the OLS scheme is superior all around, with the Lasso method a distant second and the Ridge coming in third. The ordinary scheme produces the best MSE at $\text{MSE}_\text{OLS}\simeq0.00928$.
\begin{figure}
\centering
\includegraphics[width=\linewidth]{lambda_terrain.png}
\captionof{figure}{The calculated mean squared error for the test data set described in section \ref{sect:terrain_param} for varying values of the shrinkage parameter $\lambda$. The OLS, the Ridge regression, and the Lasso scheme are all shown. \label{fig:15}}
\end{figure}


\begin{figure*}[p]
\centering
\subfloat[Test data] {\includegraphics[width=0.5\linewidth]{test_terrain.png}}
\subfloat[OLS]  {\includegraphics[width=0.5\linewidth]{olsterrain.png}} \\[-20pt]
\subfloat[Ridge]{\includegraphics[width=0.5\linewidth]{ridgeterrain.png}}
\subfloat[Lasso]{\includegraphics[width=0.5\linewidth]{lassoterrain.png}}
\captionof{figure}{Linear regression fits of a subset of the terrain data shown in \fig{2}. A $60\times60$ cutout of the original data is used as training data, while a separate equally large batch of points is used as test data for evaluating the fits. The test data itself is shown in the upper left hand corner. A shrinkage parameter $\lambda=10^{-3}$ is used for the Ridge regression, while $\lambda=10^{-4}$ is used for the Lasso scheme. We note that the OLS fit appears to best approximate the underlying data. This is also corroborated by the MSE calculations, which yields $\text{MSE}_\text{OLS}<\text{MSE}_\text{Ridge}<\text{MSE}_\text{Lasso}$.\label{fig:14}}
\end{figure*}


\section{Conclusion}
In the present work we have performed linear regression fits of real-valued functions of two variables. The Franke test function, as well as the terrain data considered in section \ref{sect:terrain_param}, both reveal that the ordinary least squares fits are superior to the regularized Ridge and Lasso fits for the present data sets. We propose that this is because we are in a bias-dominated region of the MSE, where the variance hardly matters at all for the overall error. This means that the variance-reducing regularization schemes do not benefit the MSE calculation much, giving OLS the edge. 

An alternative explanation may be that we have some errors in our code (or more fundamentally, in our understanding of the subject) which skews our results in favour of the OLS. Being fresh in the field, our confidence is not exactly super high in this regard.

%\end{multicols}
\onecolumn{
\printbibliography
}
\end{document}



